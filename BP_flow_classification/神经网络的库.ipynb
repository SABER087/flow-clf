{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module sklearn.cross_validation in sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn.cross_validation\n",
      "\n",
      "FILE\n",
      "    e:\\programdata\\anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py\n",
      "\n",
      "DESCRIPTION\n",
      "    The :mod:`sklearn.cross_validation` module includes utilities for cross-\n",
      "    validation and performance evaluation.\n",
      "\n",
      "CLASSES\n",
      "    BaseShuffleSplit(abc.NewBase)\n",
      "        ShuffleSplit\n",
      "            LabelShuffleSplit\n",
      "        StratifiedShuffleSplit\n",
      "    _BaseKFold(abc.NewBase)\n",
      "        KFold\n",
      "        LabelKFold\n",
      "        StratifiedKFold\n",
      "    _PartitionIterator(abc.NewBase)\n",
      "        LeaveOneLabelOut\n",
      "        LeaveOneOut\n",
      "        LeavePLabelOut\n",
      "        LeavePOut\n",
      "        PredefinedSplit\n",
      "    \n",
      "    class KFold(_BaseKFold)\n",
      "     |  K-Folds cross validation iterator.\n",
      "     |  \n",
      "     |  .. deprecated:: 0.18\n",
      "     |      This module will be removed in 0.20.\n",
      "     |      Use :class:`sklearn.model_selection.KFold` instead.\n",
      "     |  \n",
      "     |  Provides train/test indices to split data in train test sets. Split\n",
      "     |  dataset into k consecutive folds (without shuffling by default).\n",
      "     |  \n",
      "     |  Each fold is then used as a validation set once while the k - 1 remaining\n",
      "     |  fold(s) form the training set.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <cross_validation>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n : int\n",
      "     |      Total number of elements.\n",
      "     |  \n",
      "     |  n_folds : int, default=3\n",
      "     |      Number of folds. Must be at least 2.\n",
      "     |  \n",
      "     |  shuffle : boolean, optional\n",
      "     |      Whether to shuffle the data before splitting into batches.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional, default=None\n",
      "     |      If int, random_state is the seed used by the random number\n",
      "     |      generator; If RandomState instance, random_state is the random number\n",
      "     |      generator; If None, the random number generator is the RandomState\n",
      "     |      instance used by `np.random`. Used when ``shuffle`` == True.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.cross_validation import KFold\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
      "     |  >>> y = np.array([1, 2, 3, 4])\n",
      "     |  >>> kf = KFold(4, n_folds=2)\n",
      "     |  >>> len(kf)\n",
      "     |  2\n",
      "     |  >>> print(kf)  # doctest: +NORMALIZE_WHITESPACE\n",
      "     |  sklearn.cross_validation.KFold(n=4, n_folds=2, shuffle=False,\n",
      "     |                                 random_state=None)\n",
      "     |  >>> for train_index, test_index in kf:\n",
      "     |  ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  ...    X_train, X_test = X[train_index], X[test_index]\n",
      "     |  ...    y_train, y_test = y[train_index], y[test_index]\n",
      "     |  TRAIN: [2 3] TEST: [0 1]\n",
      "     |  TRAIN: [0 1] TEST: [2 3]\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The first n % n_folds folds have size n // n_folds + 1, other folds have\n",
      "     |  size n // n_folds.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  StratifiedKFold take label information into account to avoid building\n",
      "     |  folds with imbalanced class distributions (for binary or multiclass\n",
      "     |  classification tasks).\n",
      "     |  \n",
      "     |  LabelKFold: K-fold iterator variant with non-overlapping labels.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      KFold\n",
      "     |      _BaseKFold\n",
      "     |      abc.NewBase\n",
      "     |      _PartitionIterator\n",
      "     |      abc.NewBase\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n, n_folds=3, shuffle=False, random_state=None)\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _PartitionIterator:\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from abc.NewBase:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class LabelKFold(_BaseKFold)\n",
      "     |  K-fold iterator variant with non-overlapping labels.\n",
      "     |  \n",
      "     |  .. deprecated:: 0.18\n",
      "     |      This module will be removed in 0.20.\n",
      "     |      Use :class:`sklearn.model_selection.GroupKFold` instead.\n",
      "     |  \n",
      "     |  The same label will not appear in two different folds (the number of\n",
      "     |  distinct labels has to be at least equal to the number of folds).\n",
      "     |  \n",
      "     |  The folds are approximately balanced in the sense that the number of\n",
      "     |  distinct labels is approximately the same in each fold.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.17\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  labels : array-like with shape (n_samples, )\n",
      "     |      Contains a label for each sample.\n",
      "     |      The folds are built so that the same label does not appear in two\n",
      "     |      different folds.\n",
      "     |  \n",
      "     |  n_folds : int, default=3\n",
      "     |      Number of folds. Must be at least 2.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.cross_validation import LabelKFold\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
      "     |  >>> y = np.array([1, 2, 3, 4])\n",
      "     |  >>> labels = np.array([0, 0, 2, 2])\n",
      "     |  >>> label_kfold = LabelKFold(labels, n_folds=2)\n",
      "     |  >>> len(label_kfold)\n",
      "     |  2\n",
      "     |  >>> print(label_kfold)\n",
      "     |  sklearn.cross_validation.LabelKFold(n_labels=4, n_folds=2)\n",
      "     |  >>> for train_index, test_index in label_kfold:\n",
      "     |  ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  ...     X_train, X_test = X[train_index], X[test_index]\n",
      "     |  ...     y_train, y_test = y[train_index], y[test_index]\n",
      "     |  ...     print(X_train, X_test, y_train, y_test)\n",
      "     |  ...\n",
      "     |  TRAIN: [0 1] TEST: [2 3]\n",
      "     |  [[1 2]\n",
      "     |   [3 4]] [[5 6]\n",
      "     |   [7 8]] [1 2] [3 4]\n",
      "     |  TRAIN: [2 3] TEST: [0 1]\n",
      "     |  [[5 6]\n",
      "     |   [7 8]] [[1 2]\n",
      "     |   [3 4]] [3 4] [1 2]\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  LeaveOneLabelOut for splitting the data according to explicit,\n",
      "     |  domain-specific stratification of the dataset.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LabelKFold\n",
      "     |      _BaseKFold\n",
      "     |      abc.NewBase\n",
      "     |      _PartitionIterator\n",
      "     |      abc.NewBase\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, labels, n_folds=3)\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _PartitionIterator:\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from abc.NewBase:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class LabelShuffleSplit(ShuffleSplit)\n",
      "     |  Shuffle-Labels-Out cross-validation iterator\n",
      "     |  \n",
      "     |  .. deprecated:: 0.18\n",
      "     |      This module will be removed in 0.20.\n",
      "     |      Use :class:`sklearn.model_selection.GroupShuffleSplit` instead.\n",
      "     |  \n",
      "     |  Provides randomized train/test indices to split data according to a\n",
      "     |  third-party provided label. This label information can be used to encode\n",
      "     |  arbitrary domain specific stratifications of the samples as integers.\n",
      "     |  \n",
      "     |  For instance the labels could be the year of collection of the samples\n",
      "     |  and thus allow for cross-validation against time-based splits.\n",
      "     |  \n",
      "     |  The difference between LeavePLabelOut and LabelShuffleSplit is that\n",
      "     |  the former generates splits using all subsets of size ``p`` unique labels,\n",
      "     |  whereas LabelShuffleSplit generates a user-determined number of random\n",
      "     |  test splits, each with a user-determined fraction of unique labels.\n",
      "     |  \n",
      "     |  For example, a less computationally intensive alternative to\n",
      "     |  ``LeavePLabelOut(labels, p=10)`` would be\n",
      "     |  ``LabelShuffleSplit(labels, test_size=10, n_iter=100)``.\n",
      "     |  \n",
      "     |  Note: The parameters ``test_size`` and ``train_size`` refer to labels, and\n",
      "     |  not to samples, as in ShuffleSplit.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.17\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  labels :  array, [n_samples]\n",
      "     |      Labels of samples\n",
      "     |  \n",
      "     |  n_iter : int (default 5)\n",
      "     |      Number of re-shuffling and splitting iterations.\n",
      "     |  \n",
      "     |  test_size : float (default 0.2), int, or None\n",
      "     |      If float, should be between 0.0 and 1.0 and represent the\n",
      "     |      proportion of the labels to include in the test split. If\n",
      "     |      int, represents the absolute number of test labels. If None,\n",
      "     |      the value is automatically set to the complement of the train size.\n",
      "     |  \n",
      "     |  train_size : float, int, or None (default is None)\n",
      "     |      If float, should be between 0.0 and 1.0 and represent the\n",
      "     |      proportion of the labels to include in the train split. If\n",
      "     |      int, represents the absolute number of train labels. If None,\n",
      "     |      the value is automatically set to the complement of the test size.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional (default None)\n",
      "     |      If int, random_state is the seed used by the random number generator;\n",
      "     |      If RandomState instance, random_state is the random number generator;\n",
      "     |      If None, the random number generator is the RandomState instance used\n",
      "     |      by `np.random`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LabelShuffleSplit\n",
      "     |      ShuffleSplit\n",
      "     |      BaseShuffleSplit\n",
      "     |      abc.NewBase\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, labels, n_iter=5, test_size=0.2, train_size=None, random_state=None)\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseShuffleSplit:\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from abc.NewBase:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class LeaveOneLabelOut(_PartitionIterator)\n",
      "     |  Leave-One-Label_Out cross-validation iterator\n",
      "     |  \n",
      "     |  .. deprecated:: 0.18\n",
      "     |      This module will be removed in 0.20.\n",
      "     |      Use :class:`sklearn.model_selection.LeaveOneGroupOut` instead.\n",
      "     |  \n",
      "     |  Provides train/test indices to split data according to a third-party\n",
      "     |  provided label. This label information can be used to encode arbitrary\n",
      "     |  domain specific stratifications of the samples as integers.\n",
      "     |  \n",
      "     |  For instance the labels could be the year of collection of the samples\n",
      "     |  and thus allow for cross-validation against time-based splits.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <cross_validation>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  labels : array-like of int with shape (n_samples,)\n",
      "     |      Arbitrary domain-specific stratification of the data to be used\n",
      "     |      to draw the splits.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import cross_validation\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
      "     |  >>> y = np.array([1, 2, 1, 2])\n",
      "     |  >>> labels = np.array([1, 1, 2, 2])\n",
      "     |  >>> lol = cross_validation.LeaveOneLabelOut(labels)\n",
      "     |  >>> len(lol)\n",
      "     |  2\n",
      "     |  >>> print(lol)\n",
      "     |  sklearn.cross_validation.LeaveOneLabelOut(labels=[1 1 2 2])\n",
      "     |  >>> for train_index, test_index in lol:\n",
      "     |  ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  ...    X_train, X_test = X[train_index], X[test_index]\n",
      "     |  ...    y_train, y_test = y[train_index], y[test_index]\n",
      "     |  ...    print(X_train, X_test, y_train, y_test)\n",
      "     |  TRAIN: [2 3] TEST: [0 1]\n",
      "     |  [[5 6]\n",
      "     |   [7 8]] [[1 2]\n",
      "     |   [3 4]] [1 2] [1 2]\n",
      "     |  TRAIN: [0 1] TEST: [2 3]\n",
      "     |  [[1 2]\n",
      "     |   [3 4]] [[5 6]\n",
      "     |   [7 8]] [1 2] [1 2]\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  LabelKFold: K-fold iterator variant with non-overlapping labels.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LeaveOneLabelOut\n",
      "     |      _PartitionIterator\n",
      "     |      abc.NewBase\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, labels)\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _PartitionIterator:\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from abc.NewBase:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class LeaveOneOut(_PartitionIterator)\n",
      "     |  Leave-One-Out cross validation iterator.\n",
      "     |  \n",
      "     |  .. deprecated:: 0.18\n",
      "     |      This module will be removed in 0.20.\n",
      "     |      Use :class:`sklearn.model_selection.LeaveOneOut` instead.\n",
      "     |  \n",
      "     |  Provides train/test indices to split data in train test sets. Each\n",
      "     |  sample is used once as a test set (singleton) while the remaining\n",
      "     |  samples form the training set.\n",
      "     |  \n",
      "     |  Note: ``LeaveOneOut(n)`` is equivalent to ``KFold(n, n_folds=n)`` and\n",
      "     |  ``LeavePOut(n, p=1)``.\n",
      "     |  \n",
      "     |  Due to the high number of test sets (which is the same as the\n",
      "     |  number of samples) this cross validation method can be very costly.\n",
      "     |  For large datasets one should favor KFold, StratifiedKFold or\n",
      "     |  ShuffleSplit.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <cross_validation>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n : int\n",
      "     |      Total number of elements in dataset.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import cross_validation\n",
      "     |  >>> X = np.array([[1, 2], [3, 4]])\n",
      "     |  >>> y = np.array([1, 2])\n",
      "     |  >>> loo = cross_validation.LeaveOneOut(2)\n",
      "     |  >>> len(loo)\n",
      "     |  2\n",
      "     |  >>> print(loo)\n",
      "     |  sklearn.cross_validation.LeaveOneOut(n=2)\n",
      "     |  >>> for train_index, test_index in loo:\n",
      "     |  ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  ...    X_train, X_test = X[train_index], X[test_index]\n",
      "     |  ...    y_train, y_test = y[train_index], y[test_index]\n",
      "     |  ...    print(X_train, X_test, y_train, y_test)\n",
      "     |  TRAIN: [1] TEST: [0]\n",
      "     |  [[3 4]] [[1 2]] [2] [1]\n",
      "     |  TRAIN: [0] TEST: [1]\n",
      "     |  [[1 2]] [[3 4]] [1] [2]\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  LeaveOneLabelOut for splitting the data according to explicit,\n",
      "     |  domain-specific stratification of the dataset.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LeaveOneOut\n",
      "     |      _PartitionIterator\n",
      "     |      abc.NewBase\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _PartitionIterator:\n",
      "     |  \n",
      "     |  __init__(self, n)\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from abc.NewBase:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class LeavePLabelOut(_PartitionIterator)\n",
      "     |  Leave-P-Label_Out cross-validation iterator\n",
      "     |  \n",
      "     |  .. deprecated:: 0.18\n",
      "     |      This module will be removed in 0.20.\n",
      "     |      Use :class:`sklearn.model_selection.LeavePGroupsOut` instead.\n",
      "     |  \n",
      "     |  Provides train/test indices to split data according to a third-party\n",
      "     |  provided label. This label information can be used to encode arbitrary\n",
      "     |  domain specific stratifications of the samples as integers.\n",
      "     |  \n",
      "     |  For instance the labels could be the year of collection of the samples\n",
      "     |  and thus allow for cross-validation against time-based splits.\n",
      "     |  \n",
      "     |  The difference between LeavePLabelOut and LeaveOneLabelOut is that\n",
      "     |  the former builds the test sets with all the samples assigned to\n",
      "     |  ``p`` different values of the labels while the latter uses samples\n",
      "     |  all assigned the same labels.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <cross_validation>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  labels : array-like of int with shape (n_samples,)\n",
      "     |      Arbitrary domain-specific stratification of the data to be used\n",
      "     |      to draw the splits.\n",
      "     |  \n",
      "     |  p : int\n",
      "     |      Number of samples to leave out in the test split.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import cross_validation\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [5, 6]])\n",
      "     |  >>> y = np.array([1, 2, 1])\n",
      "     |  >>> labels = np.array([1, 2, 3])\n",
      "     |  >>> lpl = cross_validation.LeavePLabelOut(labels, p=2)\n",
      "     |  >>> len(lpl)\n",
      "     |  3\n",
      "     |  >>> print(lpl)\n",
      "     |  sklearn.cross_validation.LeavePLabelOut(labels=[1 2 3], p=2)\n",
      "     |  >>> for train_index, test_index in lpl:\n",
      "     |  ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  ...    X_train, X_test = X[train_index], X[test_index]\n",
      "     |  ...    y_train, y_test = y[train_index], y[test_index]\n",
      "     |  ...    print(X_train, X_test, y_train, y_test)\n",
      "     |  TRAIN: [2] TEST: [0 1]\n",
      "     |  [[5 6]] [[1 2]\n",
      "     |   [3 4]] [1] [1 2]\n",
      "     |  TRAIN: [1] TEST: [0 2]\n",
      "     |  [[3 4]] [[1 2]\n",
      "     |   [5 6]] [2] [1 1]\n",
      "     |  TRAIN: [0] TEST: [1 2]\n",
      "     |  [[1 2]] [[3 4]\n",
      "     |   [5 6]] [1] [2 1]\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  LabelKFold: K-fold iterator variant with non-overlapping labels.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LeavePLabelOut\n",
      "     |      _PartitionIterator\n",
      "     |      abc.NewBase\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, labels, p)\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _PartitionIterator:\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from abc.NewBase:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class LeavePOut(_PartitionIterator)\n",
      "     |  Leave-P-Out cross validation iterator\n",
      "     |  \n",
      "     |  .. deprecated:: 0.18\n",
      "     |      This module will be removed in 0.20.\n",
      "     |      Use :class:`sklearn.model_selection.LeavePOut` instead.\n",
      "     |  \n",
      "     |  Provides train/test indices to split data in train test sets. This results\n",
      "     |  in testing on all distinct samples of size p, while the remaining n - p\n",
      "     |  samples form the training set in each iteration.\n",
      "     |  \n",
      "     |  Note: ``LeavePOut(n, p)`` is NOT equivalent to ``KFold(n, n_folds=n // p)``\n",
      "     |  which creates non-overlapping test sets.\n",
      "     |  \n",
      "     |  Due to the high number of iterations which grows combinatorically with the\n",
      "     |  number of samples this cross validation method can be very costly. For\n",
      "     |  large datasets one should favor KFold, StratifiedKFold or ShuffleSplit.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <cross_validation>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n : int\n",
      "     |      Total number of elements in dataset.\n",
      "     |  \n",
      "     |  p : int\n",
      "     |      Size of the test sets.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import cross_validation\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
      "     |  >>> y = np.array([1, 2, 3, 4])\n",
      "     |  >>> lpo = cross_validation.LeavePOut(4, 2)\n",
      "     |  >>> len(lpo)\n",
      "     |  6\n",
      "     |  >>> print(lpo)\n",
      "     |  sklearn.cross_validation.LeavePOut(n=4, p=2)\n",
      "     |  >>> for train_index, test_index in lpo:\n",
      "     |  ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  ...    X_train, X_test = X[train_index], X[test_index]\n",
      "     |  ...    y_train, y_test = y[train_index], y[test_index]\n",
      "     |  TRAIN: [2 3] TEST: [0 1]\n",
      "     |  TRAIN: [1 3] TEST: [0 2]\n",
      "     |  TRAIN: [1 2] TEST: [0 3]\n",
      "     |  TRAIN: [0 3] TEST: [1 2]\n",
      "     |  TRAIN: [0 2] TEST: [1 3]\n",
      "     |  TRAIN: [0 1] TEST: [2 3]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LeavePOut\n",
      "     |      _PartitionIterator\n",
      "     |      abc.NewBase\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n, p)\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _PartitionIterator:\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from abc.NewBase:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class PredefinedSplit(_PartitionIterator)\n",
      "     |  Predefined split cross validation iterator\n",
      "     |  \n",
      "     |  .. deprecated:: 0.18\n",
      "     |      This module will be removed in 0.20.\n",
      "     |      Use :class:`sklearn.model_selection.PredefinedSplit` instead.\n",
      "     |  \n",
      "     |  Splits the data into training/test set folds according to a predefined\n",
      "     |  scheme. Each sample can be assigned to at most one test set fold, as\n",
      "     |  specified by the user through the ``test_fold`` parameter.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <cross_validation>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  test_fold : \"array-like, shape (n_samples,)\n",
      "     |      test_fold[i] gives the test set fold of sample i. A value of -1\n",
      "     |      indicates that the corresponding sample is not part of any test set\n",
      "     |      folds, but will instead always be put into the training fold.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.cross_validation import PredefinedSplit\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
      "     |  >>> y = np.array([0, 0, 1, 1])\n",
      "     |  >>> ps = PredefinedSplit(test_fold=[0, 1, -1, 1])\n",
      "     |  >>> len(ps)\n",
      "     |  2\n",
      "     |  >>> print(ps)       # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n",
      "     |  sklearn.cross_validation.PredefinedSplit(test_fold=[ 0  1 -1  1])\n",
      "     |  >>> for train_index, test_index in ps:\n",
      "     |  ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  ...    X_train, X_test = X[train_index], X[test_index]\n",
      "     |  ...    y_train, y_test = y[train_index], y[test_index]\n",
      "     |  TRAIN: [1 2 3] TEST: [0]\n",
      "     |  TRAIN: [0 2] TEST: [1 3]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PredefinedSplit\n",
      "     |      _PartitionIterator\n",
      "     |      abc.NewBase\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, test_fold)\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _PartitionIterator:\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from abc.NewBase:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class ShuffleSplit(BaseShuffleSplit)\n",
      "     |  Random permutation cross-validation iterator.\n",
      "     |  \n",
      "     |  .. deprecated:: 0.18\n",
      "     |      This module will be removed in 0.20.\n",
      "     |      Use :class:`sklearn.model_selection.ShuffleSplit` instead.\n",
      "     |  \n",
      "     |  Yields indices to split data into training and test sets.\n",
      "     |  \n",
      "     |  Note: contrary to other cross-validation strategies, random splits\n",
      "     |  do not guarantee that all folds will be different, although this is\n",
      "     |  still very likely for sizeable datasets.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <cross_validation>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n : int\n",
      "     |      Total number of elements in the dataset.\n",
      "     |  \n",
      "     |  n_iter : int (default 10)\n",
      "     |      Number of re-shuffling & splitting iterations.\n",
      "     |  \n",
      "     |  test_size : float (default 0.1), int, or None\n",
      "     |      If float, should be between 0.0 and 1.0 and represent the\n",
      "     |      proportion of the dataset to include in the test split. If\n",
      "     |      int, represents the absolute number of test samples. If None,\n",
      "     |      the value is automatically set to the complement of the train size.\n",
      "     |  \n",
      "     |  train_size : float, int, or None (default is None)\n",
      "     |      If float, should be between 0.0 and 1.0 and represent the\n",
      "     |      proportion of the dataset to include in the train split. If\n",
      "     |      int, represents the absolute number of train samples. If None,\n",
      "     |      the value is automatically set to the complement of the test size.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional (default None)\n",
      "     |      If int, random_state is the seed used by the random number generator;\n",
      "     |      If RandomState instance, random_state is the random number generator;\n",
      "     |      If None, the random number generator is the RandomState instance used\n",
      "     |      by `np.random`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import cross_validation\n",
      "     |  >>> rs = cross_validation.ShuffleSplit(4, n_iter=3,\n",
      "     |  ...     test_size=.25, random_state=0)\n",
      "     |  >>> len(rs)\n",
      "     |  3\n",
      "     |  >>> print(rs)\n",
      "     |  ... # doctest: +ELLIPSIS\n",
      "     |  ShuffleSplit(4, n_iter=3, test_size=0.25, ...)\n",
      "     |  >>> for train_index, test_index in rs:\n",
      "     |  ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  ...\n",
      "     |  TRAIN: [3 1 0] TEST: [2]\n",
      "     |  TRAIN: [2 1 3] TEST: [0]\n",
      "     |  TRAIN: [0 2 1] TEST: [3]\n",
      "     |  \n",
      "     |  >>> rs = cross_validation.ShuffleSplit(4, n_iter=3,\n",
      "     |  ...     train_size=0.5, test_size=.25, random_state=0)\n",
      "     |  >>> for train_index, test_index in rs:\n",
      "     |  ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  ...\n",
      "     |  TRAIN: [3 1] TEST: [2]\n",
      "     |  TRAIN: [2 1] TEST: [0]\n",
      "     |  TRAIN: [0 2] TEST: [3]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ShuffleSplit\n",
      "     |      BaseShuffleSplit\n",
      "     |      abc.NewBase\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseShuffleSplit:\n",
      "     |  \n",
      "     |  __init__(self, n, n_iter=10, test_size=0.1, train_size=None, random_state=None)\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from abc.NewBase:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class StratifiedKFold(_BaseKFold)\n",
      "     |  Stratified K-Folds cross validation iterator\n",
      "     |  \n",
      "     |  .. deprecated:: 0.18\n",
      "     |      This module will be removed in 0.20.\n",
      "     |      Use :class:`sklearn.model_selection.StratifiedKFold` instead.\n",
      "     |  \n",
      "     |  Provides train/test indices to split data in train test sets.\n",
      "     |  \n",
      "     |  This cross-validation object is a variation of KFold that\n",
      "     |  returns stratified folds. The folds are made by preserving\n",
      "     |  the percentage of samples for each class.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <cross_validation>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  y : array-like, [n_samples]\n",
      "     |      Samples to split in K folds.\n",
      "     |  \n",
      "     |  n_folds : int, default=3\n",
      "     |      Number of folds. Must be at least 2.\n",
      "     |  \n",
      "     |  shuffle : boolean, optional\n",
      "     |      Whether to shuffle each stratification of the data before splitting\n",
      "     |      into batches.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional, default=None\n",
      "     |      If int, random_state is the seed used by the random number\n",
      "     |      generator; If RandomState instance, random_state is the random number\n",
      "     |      generator; If None, the random number generator is the RandomState\n",
      "     |      instance used by `np.random`. Used when ``shuffle`` == True.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.cross_validation import StratifiedKFold\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
      "     |  >>> y = np.array([0, 0, 1, 1])\n",
      "     |  >>> skf = StratifiedKFold(y, n_folds=2)\n",
      "     |  >>> len(skf)\n",
      "     |  2\n",
      "     |  >>> print(skf)  # doctest: +NORMALIZE_WHITESPACE\n",
      "     |  sklearn.cross_validation.StratifiedKFold(labels=[0 0 1 1], n_folds=2,\n",
      "     |                                           shuffle=False, random_state=None)\n",
      "     |  >>> for train_index, test_index in skf:\n",
      "     |  ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  ...    X_train, X_test = X[train_index], X[test_index]\n",
      "     |  ...    y_train, y_test = y[train_index], y[test_index]\n",
      "     |  TRAIN: [1 3] TEST: [0 2]\n",
      "     |  TRAIN: [0 2] TEST: [1 3]\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  All the folds have size trunc(n_samples / n_folds), the last one has the\n",
      "     |  complementary.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  LabelKFold: K-fold iterator variant with non-overlapping labels.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      StratifiedKFold\n",
      "     |      _BaseKFold\n",
      "     |      abc.NewBase\n",
      "     |      _PartitionIterator\n",
      "     |      abc.NewBase\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, y, n_folds=3, shuffle=False, random_state=None)\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _PartitionIterator:\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from abc.NewBase:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class StratifiedShuffleSplit(BaseShuffleSplit)\n",
      "     |  Stratified ShuffleSplit cross validation iterator\n",
      "     |  \n",
      "     |  .. deprecated:: 0.18\n",
      "     |      This module will be removed in 0.20.\n",
      "     |      Use :class:`sklearn.model_selection.StratifiedShuffleSplit` instead.\n",
      "     |  \n",
      "     |  Provides train/test indices to split data in train test sets.\n",
      "     |  \n",
      "     |  This cross-validation object is a merge of StratifiedKFold and\n",
      "     |  ShuffleSplit, which returns stratified randomized folds. The folds\n",
      "     |  are made by preserving the percentage of samples for each class.\n",
      "     |  \n",
      "     |  Note: like the ShuffleSplit strategy, stratified random splits\n",
      "     |  do not guarantee that all folds will be different, although this is\n",
      "     |  still very likely for sizeable datasets.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <cross_validation>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  y : array, [n_samples]\n",
      "     |      Labels of samples.\n",
      "     |  \n",
      "     |  n_iter : int (default 10)\n",
      "     |      Number of re-shuffling & splitting iterations.\n",
      "     |  \n",
      "     |  test_size : float (default 0.1), int, or None\n",
      "     |      If float, should be between 0.0 and 1.0 and represent the\n",
      "     |      proportion of the dataset to include in the test split. If\n",
      "     |      int, represents the absolute number of test samples. If None,\n",
      "     |      the value is automatically set to the complement of the train size.\n",
      "     |  \n",
      "     |  train_size : float, int, or None (default is None)\n",
      "     |      If float, should be between 0.0 and 1.0 and represent the\n",
      "     |      proportion of the dataset to include in the train split. If\n",
      "     |      int, represents the absolute number of train samples. If None,\n",
      "     |      the value is automatically set to the complement of the test size.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional (default None)\n",
      "     |      If int, random_state is the seed used by the random number generator;\n",
      "     |      If RandomState instance, random_state is the random number generator;\n",
      "     |      If None, the random number generator is the RandomState instance used\n",
      "     |      by `np.random`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.cross_validation import StratifiedShuffleSplit\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
      "     |  >>> y = np.array([0, 0, 1, 1])\n",
      "     |  >>> sss = StratifiedShuffleSplit(y, 3, test_size=0.5, random_state=0)\n",
      "     |  >>> len(sss)\n",
      "     |  3\n",
      "     |  >>> print(sss)       # doctest: +ELLIPSIS\n",
      "     |  StratifiedShuffleSplit(labels=[0 0 1 1], n_iter=3, ...)\n",
      "     |  >>> for train_index, test_index in sss:\n",
      "     |  ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  ...    X_train, X_test = X[train_index], X[test_index]\n",
      "     |  ...    y_train, y_test = y[train_index], y[test_index]\n",
      "     |  TRAIN: [1 2] TEST: [3 0]\n",
      "     |  TRAIN: [0 2] TEST: [1 3]\n",
      "     |  TRAIN: [0 2] TEST: [3 1]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      StratifiedShuffleSplit\n",
      "     |      BaseShuffleSplit\n",
      "     |      abc.NewBase\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, y, n_iter=10, test_size=0.1, train_size=None, random_state=None)\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseShuffleSplit:\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from abc.NewBase:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    check_cv(cv, X=None, y=None, classifier=False)\n",
      "        Input checker utility for building a CV in a user friendly way.\n",
      "        \n",
      "        .. deprecated:: 0.18\n",
      "            This module will be removed in 0.20.\n",
      "            Use :func:`sklearn.model_selection.check_cv` instead.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cv : int, cross-validation generator or an iterable, optional\n",
      "            Determines the cross-validation splitting strategy.\n",
      "            Possible inputs for cv are:\n",
      "        \n",
      "            - None, to use the default 3-fold cross-validation,\n",
      "            - integer, to specify the number of folds.\n",
      "            - An object to be used as a cross-validation generator.\n",
      "            - An iterable yielding train/test splits.\n",
      "        \n",
      "            For integer/None inputs, if classifier is True and ``y`` is binary or\n",
      "            multiclass, :class:`StratifiedKFold` is used. In all other cases,\n",
      "            :class:`KFold` is used.\n",
      "        \n",
      "            Refer :ref:`User Guide <cross_validation>` for the various\n",
      "            cross-validation strategies that can be used here.\n",
      "        \n",
      "        X : array-like\n",
      "            The data the cross-val object will be applied on.\n",
      "        \n",
      "        y : array-like\n",
      "            The target variable for a supervised learning problem.\n",
      "        \n",
      "        classifier : boolean optional\n",
      "            Whether the task is a classification task, in which case\n",
      "            stratified KFold will be used.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        checked_cv : a cross-validation generator instance.\n",
      "            The return value is guaranteed to be a cv generator instance, whatever\n",
      "            the input type.\n",
      "    \n",
      "    cross_val_predict(estimator, X, y=None, cv=None, n_jobs=1, verbose=0, fit_params=None, pre_dispatch='2*n_jobs')\n",
      "        Generate cross-validated estimates for each input data point\n",
      "        \n",
      "        .. deprecated:: 0.18\n",
      "            This module will be removed in 0.20.\n",
      "            Use :func:`sklearn.model_selection.cross_val_predict` instead.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <cross_validation>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : estimator object implementing 'fit' and 'predict'\n",
      "            The object to use to fit the data.\n",
      "        \n",
      "        X : array-like\n",
      "            The data to fit. Can be, for example a list, or an array at least 2d.\n",
      "        \n",
      "        y : array-like, optional, default: None\n",
      "            The target variable to try to predict in the case of\n",
      "            supervised learning.\n",
      "        \n",
      "        cv : int, cross-validation generator or an iterable, optional\n",
      "            Determines the cross-validation splitting strategy.\n",
      "            Possible inputs for cv are:\n",
      "        \n",
      "            - None, to use the default 3-fold cross-validation,\n",
      "            - integer, to specify the number of folds.\n",
      "            - An object to be used as a cross-validation generator.\n",
      "            - An iterable yielding train/test splits.\n",
      "        \n",
      "            For integer/None inputs, if the estimator is a classifier and ``y`` is\n",
      "            either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      "            other cases, :class:`KFold` is used.\n",
      "        \n",
      "            Refer :ref:`User Guide <cross_validation>` for the various\n",
      "            cross-validation strategies that can be used here.\n",
      "        \n",
      "        n_jobs : integer, optional\n",
      "            The number of CPUs to use to do the computation. -1 means\n",
      "            'all CPUs'.\n",
      "        \n",
      "        verbose : integer, optional\n",
      "            The verbosity level.\n",
      "        \n",
      "        fit_params : dict, optional\n",
      "            Parameters to pass to the fit method of the estimator.\n",
      "        \n",
      "        pre_dispatch : int, or string, optional\n",
      "            Controls the number of jobs that get dispatched during parallel\n",
      "            execution. Reducing this number can be useful to avoid an\n",
      "            explosion of memory consumption when more jobs get dispatched\n",
      "            than CPUs can process. This parameter can be:\n",
      "        \n",
      "                - None, in which case all the jobs are immediately\n",
      "                  created and spawned. Use this for lightweight and\n",
      "                  fast-running jobs, to avoid delays due to on-demand\n",
      "                  spawning of the jobs\n",
      "        \n",
      "                - An int, giving the exact number of total jobs that are\n",
      "                  spawned\n",
      "        \n",
      "                - A string, giving an expression as a function of n_jobs,\n",
      "                  as in '2*n_jobs'\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        preds : ndarray\n",
      "            This is the result of calling 'predict'\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn import datasets, linear_model\n",
      "        >>> from sklearn.cross_validation import cross_val_predict\n",
      "        >>> diabetes = datasets.load_diabetes()\n",
      "        >>> X = diabetes.data[:150]\n",
      "        >>> y = diabetes.target[:150]\n",
      "        >>> lasso = linear_model.Lasso()\n",
      "        >>> y_pred = cross_val_predict(lasso, X, y)\n",
      "    \n",
      "    cross_val_score(estimator, X, y=None, scoring=None, cv=None, n_jobs=1, verbose=0, fit_params=None, pre_dispatch='2*n_jobs')\n",
      "        Evaluate a score by cross-validation\n",
      "        \n",
      "        .. deprecated:: 0.18\n",
      "            This module will be removed in 0.20.\n",
      "            Use :func:`sklearn.model_selection.cross_val_score` instead.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <cross_validation>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : estimator object implementing 'fit'\n",
      "            The object to use to fit the data.\n",
      "        \n",
      "        X : array-like\n",
      "            The data to fit. Can be, for example a list, or an array at least 2d.\n",
      "        \n",
      "        y : array-like, optional, default: None\n",
      "            The target variable to try to predict in the case of\n",
      "            supervised learning.\n",
      "        \n",
      "        scoring : string, callable or None, optional, default: None\n",
      "            A string (see model evaluation documentation) or\n",
      "            a scorer callable object / function with signature\n",
      "            ``scorer(estimator, X, y)``.\n",
      "        \n",
      "        cv : int, cross-validation generator or an iterable, optional\n",
      "            Determines the cross-validation splitting strategy.\n",
      "            Possible inputs for cv are:\n",
      "        \n",
      "            - None, to use the default 3-fold cross-validation,\n",
      "            - integer, to specify the number of folds.\n",
      "            - An object to be used as a cross-validation generator.\n",
      "            - An iterable yielding train/test splits.\n",
      "        \n",
      "            For integer/None inputs, if the estimator is a classifier and ``y`` is\n",
      "            either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      "            other cases, :class:`KFold` is used.\n",
      "        \n",
      "            Refer :ref:`User Guide <cross_validation>` for the various\n",
      "            cross-validation strategies that can be used here.\n",
      "        \n",
      "        n_jobs : integer, optional\n",
      "            The number of CPUs to use to do the computation. -1 means\n",
      "            'all CPUs'.\n",
      "        \n",
      "        verbose : integer, optional\n",
      "            The verbosity level.\n",
      "        \n",
      "        fit_params : dict, optional\n",
      "            Parameters to pass to the fit method of the estimator.\n",
      "        \n",
      "        pre_dispatch : int, or string, optional\n",
      "            Controls the number of jobs that get dispatched during parallel\n",
      "            execution. Reducing this number can be useful to avoid an\n",
      "            explosion of memory consumption when more jobs get dispatched\n",
      "            than CPUs can process. This parameter can be:\n",
      "        \n",
      "                - None, in which case all the jobs are immediately\n",
      "                  created and spawned. Use this for lightweight and\n",
      "                  fast-running jobs, to avoid delays due to on-demand\n",
      "                  spawning of the jobs\n",
      "        \n",
      "                - An int, giving the exact number of total jobs that are\n",
      "                  spawned\n",
      "        \n",
      "                - A string, giving an expression as a function of n_jobs,\n",
      "                  as in '2*n_jobs'\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scores : array of float, shape=(len(list(cv)),)\n",
      "            Array of scores of the estimator for each run of the cross validation.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn import datasets, linear_model\n",
      "        >>> from sklearn.cross_validation import cross_val_score\n",
      "        >>> diabetes = datasets.load_diabetes()\n",
      "        >>> X = diabetes.data[:150]\n",
      "        >>> y = diabetes.target[:150]\n",
      "        >>> lasso = linear_model.Lasso()\n",
      "        >>> print(cross_val_score(lasso, X, y))  # doctest:  +ELLIPSIS\n",
      "        [ 0.33150734  0.08022311  0.03531764]\n",
      "        \n",
      "        See Also\n",
      "        ---------\n",
      "        :func:`sklearn.metrics.make_scorer`:\n",
      "            Make a scorer from a performance metric or loss function.\n",
      "    \n",
      "    permutation_test_score(estimator, X, y, cv=None, n_permutations=100, n_jobs=1, labels=None, random_state=0, verbose=0, scoring=None)\n",
      "        Evaluate the significance of a cross-validated score with permutations\n",
      "        \n",
      "        .. deprecated:: 0.18\n",
      "            This module will be removed in 0.20.\n",
      "            Use :func:`sklearn.model_selection.permutation_test_score` instead.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <cross_validation>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : estimator object implementing 'fit'\n",
      "            The object to use to fit the data.\n",
      "        \n",
      "        X : array-like of shape at least 2D\n",
      "            The data to fit.\n",
      "        \n",
      "        y : array-like\n",
      "            The target variable to try to predict in the case of\n",
      "            supervised learning.\n",
      "        \n",
      "        scoring : string, callable or None, optional, default: None\n",
      "            A string (see model evaluation documentation) or\n",
      "            a scorer callable object / function with signature\n",
      "            ``scorer(estimator, X, y)``.\n",
      "        \n",
      "        cv : int, cross-validation generator or an iterable, optional\n",
      "            Determines the cross-validation splitting strategy.\n",
      "            Possible inputs for cv are:\n",
      "        \n",
      "            - None, to use the default 3-fold cross-validation,\n",
      "            - integer, to specify the number of folds.\n",
      "            - An object to be used as a cross-validation generator.\n",
      "            - An iterable yielding train/test splits.\n",
      "        \n",
      "            For integer/None inputs, if the estimator is a classifier and ``y`` is\n",
      "            either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      "            other cases, :class:`KFold` is used.\n",
      "        \n",
      "            Refer :ref:`User Guide <cross_validation>` for the various\n",
      "            cross-validation strategies that can be used here.\n",
      "        \n",
      "        n_permutations : integer, optional\n",
      "            Number of times to permute ``y``.\n",
      "        \n",
      "        n_jobs : integer, optional\n",
      "            The number of CPUs to use to do the computation. -1 means\n",
      "            'all CPUs'.\n",
      "        \n",
      "        labels : array-like of shape [n_samples] (optional)\n",
      "            Labels constrain the permutation among groups of samples with\n",
      "            a same label.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, optional (default=0)\n",
      "            If int, random_state is the seed used by the random number generator;\n",
      "            If RandomState instance, random_state is the random number generator;\n",
      "            If None, the random number generator is the RandomState instance used\n",
      "            by `np.random`.\n",
      "        \n",
      "        verbose : integer, optional\n",
      "            The verbosity level.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            The true score without permuting targets.\n",
      "        \n",
      "        permutation_scores : array, shape (n_permutations,)\n",
      "            The scores obtained for each permutations.\n",
      "        \n",
      "        pvalue : float\n",
      "            The p-value, which approximates the probability that the score would\n",
      "            be obtained by chance. This is calculated as:\n",
      "        \n",
      "            `(C + 1) / (n_permutations + 1)`\n",
      "        \n",
      "            Where C is the number of permutations whose score >= the true score.\n",
      "        \n",
      "            The best possible p-value is 1/(n_permutations + 1), the worst is 1.0.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function implements Test 1 in:\n",
      "        \n",
      "            Ojala and Garriga. Permutation Tests for Studying Classifier\n",
      "            Performance.  The Journal of Machine Learning Research (2010)\n",
      "            vol. 11\n",
      "    \n",
      "    train_test_split(*arrays, **options)\n",
      "        Split arrays or matrices into random train and test subsets\n",
      "        \n",
      "        .. deprecated:: 0.18\n",
      "            This module will be removed in 0.20.\n",
      "            Use :func:`sklearn.model_selection.train_test_split` instead.\n",
      "        \n",
      "        Quick utility that wraps input validation and\n",
      "        ``next(iter(ShuffleSplit(n_samples)))`` and application to input\n",
      "        data into a single call for splitting (and optionally subsampling)\n",
      "        data in a oneliner.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <cross_validation>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        *arrays : sequence of indexables with same length / shape[0]\n",
      "            Allowed inputs are lists, numpy arrays, scipy-sparse\n",
      "            matrices or pandas dataframes.\n",
      "        \n",
      "        test_size : float, int, or None (default is None)\n",
      "            If float, should be between 0.0 and 1.0 and represent the\n",
      "            proportion of the dataset to include in the test split. If\n",
      "            int, represents the absolute number of test samples. If None,\n",
      "            the value is automatically set to the complement of the train size.\n",
      "            If train size is also None, test size is set to 0.25.\n",
      "        \n",
      "        train_size : float, int, or None (default is None)\n",
      "            If float, should be between 0.0 and 1.0 and represent the\n",
      "            proportion of the dataset to include in the train split. If\n",
      "            int, represents the absolute number of train samples. If None,\n",
      "            the value is automatically set to the complement of the test size.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, optional (default=None)\n",
      "            If int, random_state is the seed used by the random number generator;\n",
      "            If RandomState instance, random_state is the random number generator;\n",
      "            If None, the random number generator is the RandomState instance used\n",
      "            by `np.random`.\n",
      "        \n",
      "        stratify : array-like or None (default is None)\n",
      "            If not None, data is split in a stratified fashion, using this as\n",
      "            the labels array.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "               *stratify* splitting\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        splitting : list, length = 2 * len(arrays),\n",
      "            List containing train-test split of inputs.\n",
      "        \n",
      "            .. versionadded:: 0.16\n",
      "                If the input is sparse, the output will be a\n",
      "                ``scipy.sparse.csr_matrix``. Else, output type is the same as the\n",
      "                input type.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.cross_validation import train_test_split\n",
      "        >>> X, y = np.arange(10).reshape((5, 2)), range(5)\n",
      "        >>> X\n",
      "        array([[0, 1],\n",
      "               [2, 3],\n",
      "               [4, 5],\n",
      "               [6, 7],\n",
      "               [8, 9]])\n",
      "        >>> list(y)\n",
      "        [0, 1, 2, 3, 4]\n",
      "        \n",
      "        >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "        ...     X, y, test_size=0.33, random_state=42)\n",
      "        ...\n",
      "        >>> X_train\n",
      "        array([[4, 5],\n",
      "               [0, 1],\n",
      "               [6, 7]])\n",
      "        >>> y_train\n",
      "        [2, 0, 3]\n",
      "        >>> X_test\n",
      "        array([[2, 3],\n",
      "               [8, 9]])\n",
      "        >>> y_test\n",
      "        [1, 4]\n",
      "\n",
      "DATA\n",
      "    __all__ = ['KFold', 'LabelKFold', 'LeaveOneLabelOut', 'LeaveOneOut', '...\n",
      "    __warningregistry__ = {}\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramData\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sklearn.cross_validation\n",
    "help(sklearn.cross_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络的库如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn.neural_network in sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn.neural_network\n",
      "\n",
      "FILE\n",
      "    e:\\programdata\\anaconda2\\lib\\site-packages\\sklearn\\neural_network\\__init__.py\n",
      "\n",
      "DESCRIPTION\n",
      "    The :mod:`sklearn.neural_network` module includes models based on neural\n",
      "    networks.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _base\n",
      "    _stochastic_optimizers\n",
      "    multilayer_perceptron\n",
      "    rbm\n",
      "    tests (package)\n",
      "\n",
      "CLASSES\n",
      "    sklearn.base.BaseEstimator(__builtin__.object)\n",
      "        sklearn.neural_network.rbm.BernoulliRBM(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin)\n",
      "    sklearn.base.ClassifierMixin(__builtin__.object)\n",
      "        sklearn.neural_network.multilayer_perceptron.MLPClassifier(sklearn.neural_network.multilayer_perceptron.BaseMultilayerPerceptron, sklearn.base.ClassifierMixin)\n",
      "    sklearn.base.RegressorMixin(__builtin__.object)\n",
      "        sklearn.neural_network.multilayer_perceptron.MLPRegressor(sklearn.neural_network.multilayer_perceptron.BaseMultilayerPerceptron, sklearn.base.RegressorMixin)\n",
      "    sklearn.base.TransformerMixin(__builtin__.object)\n",
      "        sklearn.neural_network.rbm.BernoulliRBM(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin)\n",
      "    sklearn.neural_network.multilayer_perceptron.BaseMultilayerPerceptron(abc.NewBase)\n",
      "        sklearn.neural_network.multilayer_perceptron.MLPClassifier(sklearn.neural_network.multilayer_perceptron.BaseMultilayerPerceptron, sklearn.base.ClassifierMixin)\n",
      "        sklearn.neural_network.multilayer_perceptron.MLPRegressor(sklearn.neural_network.multilayer_perceptron.BaseMultilayerPerceptron, sklearn.base.RegressorMixin)\n",
      "    \n",
      "    class BernoulliRBM(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin)\n",
      "     |  Bernoulli Restricted Boltzmann Machine (RBM).\n",
      "     |  \n",
      "     |  A Restricted Boltzmann Machine with binary visible units and\n",
      "     |  binary hidden units. Parameters are estimated using Stochastic Maximum\n",
      "     |  Likelihood (SML), also known as Persistent Contrastive Divergence (PCD)\n",
      "     |  [2].\n",
      "     |  \n",
      "     |  The time complexity of this implementation is ``O(d ** 2)`` assuming\n",
      "     |  d ~ n_features ~ n_components.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <rbm>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_components : int, optional\n",
      "     |      Number of binary hidden units.\n",
      "     |  \n",
      "     |  learning_rate : float, optional\n",
      "     |      The learning rate for weight updates. It is *highly* recommended\n",
      "     |      to tune this hyper-parameter. Reasonable values are in the\n",
      "     |      10**[0., -3.] range.\n",
      "     |  \n",
      "     |  batch_size : int, optional\n",
      "     |      Number of examples per minibatch.\n",
      "     |  \n",
      "     |  n_iter : int, optional\n",
      "     |      Number of iterations/sweeps over the training dataset to perform\n",
      "     |      during training.\n",
      "     |  \n",
      "     |  verbose : int, optional\n",
      "     |      The verbosity level. The default, zero, means silent mode.\n",
      "     |  \n",
      "     |  random_state : integer or numpy.RandomState, optional\n",
      "     |      A random number generator instance to define the state of the\n",
      "     |      random permutations generator. If an integer is given, it fixes the\n",
      "     |      seed. Defaults to the global numpy random number generator.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  intercept_hidden_ : array-like, shape (n_components,)\n",
      "     |      Biases of the hidden units.\n",
      "     |  \n",
      "     |  intercept_visible_ : array-like, shape (n_features,)\n",
      "     |      Biases of the visible units.\n",
      "     |  \n",
      "     |  components_ : array-like, shape (n_components, n_features)\n",
      "     |      Weight matrix, where n_features in the number of\n",
      "     |      visible units and n_components is the number of hidden units.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  \n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.neural_network import BernoulliRBM\n",
      "     |  >>> X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
      "     |  >>> model = BernoulliRBM(n_components=2)\n",
      "     |  >>> model.fit(X)\n",
      "     |  BernoulliRBM(batch_size=10, learning_rate=0.1, n_components=2, n_iter=10,\n",
      "     |         random_state=None, verbose=0)\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  [1] Hinton, G. E., Osindero, S. and Teh, Y. A fast learning algorithm for\n",
      "     |      deep belief nets. Neural Computation 18, pp 1527-1554.\n",
      "     |      http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf\n",
      "     |  \n",
      "     |  [2] Tieleman, T. Training Restricted Boltzmann Machines using\n",
      "     |      Approximations to the Likelihood Gradient. International Conference\n",
      "     |      on Machine Learning (ICML) 2008\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BernoulliRBM\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_components=256, learning_rate=0.1, batch_size=10, n_iter=10, verbose=0, random_state=None)\n",
      "     |  \n",
      "     |  fit(self, X, y=None)\n",
      "     |      Fit the model to the data X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : BernoulliRBM\n",
      "     |          The fitted model.\n",
      "     |  \n",
      "     |  gibbs(self, v)\n",
      "     |      Perform one Gibbs sampling step.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      v : array-like, shape (n_samples, n_features)\n",
      "     |          Values of the visible layer to start from.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      v_new : array-like, shape (n_samples, n_features)\n",
      "     |          Values of the visible layer after one Gibbs step.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y=None)\n",
      "     |      Fit the model to the data X which should contain a partial\n",
      "     |      segment of the data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : BernoulliRBM\n",
      "     |          The fitted model.\n",
      "     |  \n",
      "     |  score_samples(self, X)\n",
      "     |      Compute the pseudo-likelihood of X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} shape (n_samples, n_features)\n",
      "     |          Values of the visible layer. Must be all-boolean (not checked).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      pseudo_likelihood : array-like, shape (n_samples,)\n",
      "     |          Value of the pseudo-likelihood (proxy for likelihood).\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This method is not deterministic: it computes a quantity called the\n",
      "     |      free energy on X, then on a randomly corrupted version of X, and\n",
      "     |      returns the log of the logistic function of the difference.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Compute the hidden layer activation probabilities, P(h=1|v=X).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} shape (n_samples, n_features)\n",
      "     |          The data to be transformed.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      h : array, shape (n_samples, n_components)\n",
      "     |          Latent representations of the data.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to X and y with optional parameters fit_params\n",
      "     |      and returns a transformed version of X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : numpy array of shape [n_samples, n_features]\n",
      "     |          Training set.\n",
      "     |      \n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : numpy array of shape [n_samples, n_features_new]\n",
      "     |          Transformed array.\n",
      "    \n",
      "    class MLPClassifier(BaseMultilayerPerceptron, sklearn.base.ClassifierMixin)\n",
      "     |  Multi-layer Perceptron classifier.\n",
      "     |  \n",
      "     |  This model optimizes the log-loss function using LBFGS or stochastic\n",
      "     |  gradient descent.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.18\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  hidden_layer_sizes : tuple, length = n_layers - 2, default (100,)\n",
      "     |      The ith element represents the number of neurons in the ith\n",
      "     |      hidden layer.\n",
      "     |  \n",
      "     |  activation : {'identity', 'logistic', 'tanh', 'relu'}, default 'relu'\n",
      "     |      Activation function for the hidden layer.\n",
      "     |  \n",
      "     |      - 'identity', no-op activation, useful to implement linear bottleneck,\n",
      "     |        returns f(x) = x\n",
      "     |  \n",
      "     |      - 'logistic', the logistic sigmoid function,\n",
      "     |        returns f(x) = 1 / (1 + exp(-x)).\n",
      "     |  \n",
      "     |      - 'tanh', the hyperbolic tan function,\n",
      "     |        returns f(x) = tanh(x).\n",
      "     |  \n",
      "     |      - 'relu', the rectified linear unit function,\n",
      "     |        returns f(x) = max(0, x)\n",
      "     |  \n",
      "     |  solver : {'lbfgs', 'sgd', 'adam'}, default 'adam'\n",
      "     |      The solver for weight optimization.\n",
      "     |  \n",
      "     |      - 'lbfgs' is an optimizer in the family of quasi-Newton methods.\n",
      "     |  \n",
      "     |      - 'sgd' refers to stochastic gradient descent.\n",
      "     |  \n",
      "     |      - 'adam' refers to a stochastic gradient-based optimizer proposed\n",
      "     |        by Kingma, Diederik, and Jimmy Ba\n",
      "     |  \n",
      "     |      Note: The default solver 'adam' works pretty well on relatively\n",
      "     |      large datasets (with thousands of training samples or more) in terms of\n",
      "     |      both training time and validation score.\n",
      "     |      For small datasets, however, 'lbfgs' can converge faster and perform\n",
      "     |      better.\n",
      "     |  \n",
      "     |  alpha : float, optional, default 0.0001\n",
      "     |      L2 penalty (regularization term) parameter.\n",
      "     |  \n",
      "     |  batch_size : int, optional, default 'auto'\n",
      "     |      Size of minibatches for stochastic optimizers.\n",
      "     |      If the solver is 'lbfgs', the classifier will not use minibatch.\n",
      "     |      When set to \"auto\", `batch_size=min(200, n_samples)`\n",
      "     |  \n",
      "     |  learning_rate : {'constant', 'invscaling', 'adaptive'}, default 'constant'\n",
      "     |      Learning rate schedule for weight updates.\n",
      "     |  \n",
      "     |      - 'constant' is a constant learning rate given by\n",
      "     |        'learning_rate_init'.\n",
      "     |  \n",
      "     |      - 'invscaling' gradually decreases the learning rate ``learning_rate_``\n",
      "     |        at each time step 't' using an inverse scaling exponent of 'power_t'.\n",
      "     |        effective_learning_rate = learning_rate_init / pow(t, power_t)\n",
      "     |  \n",
      "     |      - 'adaptive' keeps the learning rate constant to\n",
      "     |        'learning_rate_init' as long as training loss keeps decreasing.\n",
      "     |        Each time two consecutive epochs fail to decrease training loss by at\n",
      "     |        least tol, or fail to increase validation score by at least tol if\n",
      "     |        'early_stopping' is on, the current learning rate is divided by 5.\n",
      "     |  \n",
      "     |      Only used when ``solver='sgd'``.\n",
      "     |  \n",
      "     |  learning_rate_init : double, optional, default 0.001\n",
      "     |      The initial learning rate used. It controls the step-size\n",
      "     |      in updating the weights. Only used when solver='sgd' or 'adam'.\n",
      "     |  \n",
      "     |  power_t : double, optional, default 0.5\n",
      "     |      The exponent for inverse scaling learning rate.\n",
      "     |      It is used in updating effective learning rate when the learning_rate\n",
      "     |      is set to 'invscaling'. Only used when solver='sgd'.\n",
      "     |  \n",
      "     |  max_iter : int, optional, default 200\n",
      "     |      Maximum number of iterations. The solver iterates until convergence\n",
      "     |      (determined by 'tol') or this number of iterations. For stochastic\n",
      "     |      solvers ('sgd', 'adam'), note that this determines the number of epochs\n",
      "     |      (how many times each data point will be used), not the number of\n",
      "     |      gradient steps.\n",
      "     |  \n",
      "     |  shuffle : bool, optional, default True\n",
      "     |      Whether to shuffle samples in each iteration. Only used when\n",
      "     |      solver='sgd' or 'adam'.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional, default None\n",
      "     |      If int, random_state is the seed used by the random number generator;\n",
      "     |      If RandomState instance, random_state is the random number generator;\n",
      "     |      If None, the random number generator is the RandomState instance used\n",
      "     |      by `np.random`.\n",
      "     |  \n",
      "     |  tol : float, optional, default 1e-4\n",
      "     |      Tolerance for the optimization. When the loss or score is not improving\n",
      "     |      by at least tol for two consecutive iterations, unless `learning_rate`\n",
      "     |      is set to 'adaptive', convergence is considered to be reached and\n",
      "     |      training stops.\n",
      "     |  \n",
      "     |  verbose : bool, optional, default False\n",
      "     |      Whether to print progress messages to stdout.\n",
      "     |  \n",
      "     |  warm_start : bool, optional, default False\n",
      "     |      When set to True, reuse the solution of the previous\n",
      "     |      call to fit as initialization, otherwise, just erase the\n",
      "     |      previous solution.\n",
      "     |  \n",
      "     |  momentum : float, default 0.9\n",
      "     |      Momentum for gradient descent update. Should be between 0 and 1. Only\n",
      "     |      used when solver='sgd'.\n",
      "     |  \n",
      "     |  nesterovs_momentum : boolean, default True\n",
      "     |      Whether to use Nesterov's momentum. Only used when solver='sgd' and\n",
      "     |      momentum > 0.\n",
      "     |  \n",
      "     |  early_stopping : bool, default False\n",
      "     |      Whether to use early stopping to terminate training when validation\n",
      "     |      score is not improving. If set to true, it will automatically set\n",
      "     |      aside 10% of training data as validation and terminate training when\n",
      "     |      validation score is not improving by at least tol for two consecutive\n",
      "     |      epochs.\n",
      "     |      Only effective when solver='sgd' or 'adam'\n",
      "     |  \n",
      "     |  validation_fraction : float, optional, default 0.1\n",
      "     |      The proportion of training data to set aside as validation set for\n",
      "     |      early stopping. Must be between 0 and 1.\n",
      "     |      Only used if early_stopping is True\n",
      "     |  \n",
      "     |  beta_1 : float, optional, default 0.9\n",
      "     |      Exponential decay rate for estimates of first moment vector in adam,\n",
      "     |      should be in [0, 1). Only used when solver='adam'\n",
      "     |  \n",
      "     |  beta_2 : float, optional, default 0.999\n",
      "     |      Exponential decay rate for estimates of second moment vector in adam,\n",
      "     |      should be in [0, 1). Only used when solver='adam'\n",
      "     |  \n",
      "     |  epsilon : float, optional, default 1e-8\n",
      "     |      Value for numerical stability in adam. Only used when solver='adam'\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  classes_ : array or list of array of shape (n_classes,)\n",
      "     |      Class labels for each output.\n",
      "     |  \n",
      "     |  loss_ : float\n",
      "     |      The current loss computed with the loss function.\n",
      "     |  \n",
      "     |  coefs_ : list, length n_layers - 1\n",
      "     |      The ith element in the list represents the weight matrix corresponding\n",
      "     |      to layer i.\n",
      "     |  \n",
      "     |  intercepts_ : list, length n_layers - 1\n",
      "     |      The ith element in the list represents the bias vector corresponding to\n",
      "     |      layer i + 1.\n",
      "     |  \n",
      "     |  n_iter_ : int,\n",
      "     |      The number of iterations the solver has ran.\n",
      "     |  \n",
      "     |  n_layers_ : int\n",
      "     |      Number of layers.\n",
      "     |  \n",
      "     |  n_outputs_ : int\n",
      "     |      Number of outputs.\n",
      "     |  \n",
      "     |  out_activation_ : string\n",
      "     |      Name of the output activation function.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  MLPClassifier trains iteratively since at each time step\n",
      "     |  the partial derivatives of the loss function with respect to the model\n",
      "     |  parameters are computed to update the parameters.\n",
      "     |  \n",
      "     |  It can also have a regularization term added to the loss function\n",
      "     |  that shrinks model parameters to prevent overfitting.\n",
      "     |  \n",
      "     |  This implementation works with data represented as dense numpy arrays or\n",
      "     |  sparse scipy arrays of floating point values.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  Hinton, Geoffrey E.\n",
      "     |      \"Connectionist learning procedures.\" Artificial intelligence 40.1\n",
      "     |      (1989): 185-234.\n",
      "     |  \n",
      "     |  Glorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of\n",
      "     |      training deep feedforward neural networks.\" International Conference\n",
      "     |      on Artificial Intelligence and Statistics. 2010.\n",
      "     |  \n",
      "     |  He, Kaiming, et al. \"Delving deep into rectifiers: Surpassing human-level\n",
      "     |      performance on imagenet classification.\" arXiv preprint\n",
      "     |      arXiv:1502.01852 (2015).\n",
      "     |  \n",
      "     |  Kingma, Diederik, and Jimmy Ba. \"Adam: A method for stochastic\n",
      "     |      optimization.\" arXiv preprint arXiv:1412.6980 (2014).\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MLPClassifier\n",
      "     |      BaseMultilayerPerceptron\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, hidden_layer_sizes=(100,), activation='relu', solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the model to data matrix X and target(s) y.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          The input data.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          The target values (class labels in classification, real numbers in\n",
      "     |          regression).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns a trained MLP model.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the multi-layer perceptron classifier\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          The input data.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array-like, shape (n_samples,) or (n_samples, n_classes)\n",
      "     |          The predicted classes.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Return the log of probability estimates.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          The input data.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      log_y_prob : array-like, shape (n_samples, n_classes)\n",
      "     |          The predicted log-probability of the sample for each class\n",
      "     |          in the model, where classes are ordered as they are in\n",
      "     |          `self.classes_`. Equivalent to log(predict_proba(X))\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Probability estimates.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          The input data.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_prob : array-like, shape (n_samples, n_classes)\n",
      "     |          The predicted probability of the sample for each class in the\n",
      "     |          model, where classes are ordered as they are in `self.classes_`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  partial_fit\n",
      "     |      Fit the model to data matrix X and target y.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          The input data.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,)\n",
      "     |          The target values.\n",
      "     |      \n",
      "     |      classes : array, shape (n_classes)\n",
      "     |          Classes across all calls to partial_fit.\n",
      "     |          Can be obtained via `np.unique(y_all)`, where y_all is the\n",
      "     |          target vector of the entire dataset.\n",
      "     |          This argument is required for the first call to partial_fit\n",
      "     |          and can be omitted in the subsequent calls.\n",
      "     |          Note that y doesn't need to contain all labels in `classes`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns a trained MLP model.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "    \n",
      "    class MLPRegressor(BaseMultilayerPerceptron, sklearn.base.RegressorMixin)\n",
      "     |  Multi-layer Perceptron regressor.\n",
      "     |  \n",
      "     |  This model optimizes the squared-loss using LBFGS or stochastic gradient\n",
      "     |  descent.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.18\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  hidden_layer_sizes : tuple, length = n_layers - 2, default (100,)\n",
      "     |      The ith element represents the number of neurons in the ith\n",
      "     |      hidden layer.\n",
      "     |  \n",
      "     |  activation : {'identity', 'logistic', 'tanh', 'relu'}, default 'relu'\n",
      "     |      Activation function for the hidden layer.\n",
      "     |  \n",
      "     |      - 'identity', no-op activation, useful to implement linear bottleneck,\n",
      "     |        returns f(x) = x\n",
      "     |  \n",
      "     |      - 'logistic', the logistic sigmoid function,\n",
      "     |        returns f(x) = 1 / (1 + exp(-x)).\n",
      "     |  \n",
      "     |      - 'tanh', the hyperbolic tan function,\n",
      "     |        returns f(x) = tanh(x).\n",
      "     |  \n",
      "     |      - 'relu', the rectified linear unit function,\n",
      "     |        returns f(x) = max(0, x)\n",
      "     |  \n",
      "     |  solver : {'lbfgs', 'sgd', 'adam'}, default 'adam'\n",
      "     |      The solver for weight optimization.\n",
      "     |  \n",
      "     |      - 'lbfgs' is an optimizer in the family of quasi-Newton methods.\n",
      "     |  \n",
      "     |      - 'sgd' refers to stochastic gradient descent.\n",
      "     |  \n",
      "     |      - 'adam' refers to a stochastic gradient-based optimizer proposed by\n",
      "     |        Kingma, Diederik, and Jimmy Ba\n",
      "     |  \n",
      "     |      Note: The default solver 'adam' works pretty well on relatively\n",
      "     |      large datasets (with thousands of training samples or more) in terms of\n",
      "     |      both training time and validation score.\n",
      "     |      For small datasets, however, 'lbfgs' can converge faster and perform\n",
      "     |      better.\n",
      "     |  \n",
      "     |  alpha : float, optional, default 0.0001\n",
      "     |      L2 penalty (regularization term) parameter.\n",
      "     |  \n",
      "     |  batch_size : int, optional, default 'auto'\n",
      "     |      Size of minibatches for stochastic optimizers.\n",
      "     |      If the solver is 'lbfgs', the classifier will not use minibatch.\n",
      "     |      When set to \"auto\", `batch_size=min(200, n_samples)`\n",
      "     |  \n",
      "     |  learning_rate : {'constant', 'invscaling', 'adaptive'}, default 'constant'\n",
      "     |      Learning rate schedule for weight updates.\n",
      "     |  \n",
      "     |      - 'constant' is a constant learning rate given by\n",
      "     |        'learning_rate_init'.\n",
      "     |  \n",
      "     |      - 'invscaling' gradually decreases the learning rate ``learning_rate_``\n",
      "     |        at each time step 't' using an inverse scaling exponent of 'power_t'.\n",
      "     |        effective_learning_rate = learning_rate_init / pow(t, power_t)\n",
      "     |  \n",
      "     |      - 'adaptive' keeps the learning rate constant to\n",
      "     |        'learning_rate_init' as long as training loss keeps decreasing.\n",
      "     |        Each time two consecutive epochs fail to decrease training loss by at\n",
      "     |        least tol, or fail to increase validation score by at least tol if\n",
      "     |        'early_stopping' is on, the current learning rate is divided by 5.\n",
      "     |  \n",
      "     |      Only used when solver='sgd'.\n",
      "     |  \n",
      "     |  learning_rate_init : double, optional, default 0.001\n",
      "     |      The initial learning rate used. It controls the step-size\n",
      "     |      in updating the weights. Only used when solver='sgd' or 'adam'.\n",
      "     |  \n",
      "     |  power_t : double, optional, default 0.5\n",
      "     |      The exponent for inverse scaling learning rate.\n",
      "     |      It is used in updating effective learning rate when the learning_rate\n",
      "     |      is set to 'invscaling'. Only used when solver='sgd'.\n",
      "     |  \n",
      "     |  max_iter : int, optional, default 200\n",
      "     |      Maximum number of iterations. The solver iterates until convergence\n",
      "     |      (determined by 'tol') or this number of iterations. For stochastic\n",
      "     |      solvers ('sgd', 'adam'), note that this determines the number of epochs\n",
      "     |      (how many times each data point will be used), not the number of\n",
      "     |      gradient steps.\n",
      "     |  \n",
      "     |  shuffle : bool, optional, default True\n",
      "     |      Whether to shuffle samples in each iteration. Only used when\n",
      "     |      solver='sgd' or 'adam'.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional, default None\n",
      "     |      If int, random_state is the seed used by the random number generator;\n",
      "     |      If RandomState instance, random_state is the random number generator;\n",
      "     |      If None, the random number generator is the RandomState instance used\n",
      "     |      by `np.random`.\n",
      "     |  \n",
      "     |  tol : float, optional, default 1e-4\n",
      "     |      Tolerance for the optimization. When the loss or score is not improving\n",
      "     |      by at least tol for two consecutive iterations, unless `learning_rate`\n",
      "     |      is set to 'adaptive', convergence is considered to be reached and\n",
      "     |      training stops.\n",
      "     |  \n",
      "     |  verbose : bool, optional, default False\n",
      "     |      Whether to print progress messages to stdout.\n",
      "     |  \n",
      "     |  warm_start : bool, optional, default False\n",
      "     |      When set to True, reuse the solution of the previous\n",
      "     |      call to fit as initialization, otherwise, just erase the\n",
      "     |      previous solution.\n",
      "     |  \n",
      "     |  momentum : float, default 0.9\n",
      "     |      Momentum for gradient descent update.  Should be between 0 and 1. Only\n",
      "     |      used when solver='sgd'.\n",
      "     |  \n",
      "     |  nesterovs_momentum : boolean, default True\n",
      "     |      Whether to use Nesterov's momentum. Only used when solver='sgd' and\n",
      "     |      momentum > 0.\n",
      "     |  \n",
      "     |  early_stopping : bool, default False\n",
      "     |      Whether to use early stopping to terminate training when validation\n",
      "     |      score is not improving. If set to true, it will automatically set\n",
      "     |      aside 10% of training data as validation and terminate training when\n",
      "     |      validation score is not improving by at least tol for two consecutive\n",
      "     |      epochs.\n",
      "     |      Only effective when solver='sgd' or 'adam'\n",
      "     |  \n",
      "     |  validation_fraction : float, optional, default 0.1\n",
      "     |      The proportion of training data to set aside as validation set for\n",
      "     |      early stopping. Must be between 0 and 1.\n",
      "     |      Only used if early_stopping is True\n",
      "     |  \n",
      "     |  beta_1 : float, optional, default 0.9\n",
      "     |      Exponential decay rate for estimates of first moment vector in adam,\n",
      "     |      should be in [0, 1). Only used when solver='adam'\n",
      "     |  \n",
      "     |  beta_2 : float, optional, default 0.999\n",
      "     |      Exponential decay rate for estimates of second moment vector in adam,\n",
      "     |      should be in [0, 1). Only used when solver='adam'\n",
      "     |  \n",
      "     |  epsilon : float, optional, default 1e-8\n",
      "     |      Value for numerical stability in adam. Only used when solver='adam'\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  loss_ : float\n",
      "     |      The current loss computed with the loss function.\n",
      "     |  \n",
      "     |  coefs_ : list, length n_layers - 1\n",
      "     |      The ith element in the list represents the weight matrix corresponding\n",
      "     |      to layer i.\n",
      "     |  \n",
      "     |  intercepts_ : list, length n_layers - 1\n",
      "     |      The ith element in the list represents the bias vector corresponding to\n",
      "     |      layer i + 1.\n",
      "     |  \n",
      "     |  n_iter_ : int,\n",
      "     |      The number of iterations the solver has ran.\n",
      "     |  \n",
      "     |  n_layers_ : int\n",
      "     |      Number of layers.\n",
      "     |  \n",
      "     |  n_outputs_ : int\n",
      "     |      Number of outputs.\n",
      "     |  \n",
      "     |  out_activation_ : string\n",
      "     |      Name of the output activation function.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  MLPRegressor trains iteratively since at each time step\n",
      "     |  the partial derivatives of the loss function with respect to the model\n",
      "     |  parameters are computed to update the parameters.\n",
      "     |  \n",
      "     |  It can also have a regularization term added to the loss function\n",
      "     |  that shrinks model parameters to prevent overfitting.\n",
      "     |  \n",
      "     |  This implementation works with data represented as dense and sparse numpy\n",
      "     |  arrays of floating point values.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  Hinton, Geoffrey E.\n",
      "     |      \"Connectionist learning procedures.\" Artificial intelligence 40.1\n",
      "     |      (1989): 185-234.\n",
      "     |  \n",
      "     |  Glorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of\n",
      "     |      training deep feedforward neural networks.\" International Conference\n",
      "     |      on Artificial Intelligence and Statistics. 2010.\n",
      "     |  \n",
      "     |  He, Kaiming, et al. \"Delving deep into rectifiers: Surpassing human-level\n",
      "     |      performance on imagenet classification.\" arXiv preprint\n",
      "     |      arXiv:1502.01852 (2015).\n",
      "     |  \n",
      "     |  Kingma, Diederik, and Jimmy Ba. \"Adam: A method for stochastic\n",
      "     |      optimization.\" arXiv preprint arXiv:1412.6980 (2014).\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MLPRegressor\n",
      "     |      BaseMultilayerPerceptron\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, hidden_layer_sizes=(100,), activation='relu', solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the multi-layer perceptron model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          The input data.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array-like, shape (n_samples, n_outputs)\n",
      "     |          The predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseMultilayerPerceptron:\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the model to data matrix X and target(s) y.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          The input data.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          The target values (class labels in classification, real numbers in\n",
      "     |          regression).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns a trained MLP model.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseMultilayerPerceptron:\n",
      "     |  \n",
      "     |  partial_fit\n",
      "     |      Fit the model to data matrix X and target y.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          The input data.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,)\n",
      "     |          The target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns a trained MLP model.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "\n",
      "DATA\n",
      "    __all__ = ['BernoulliRBM', 'MLPClassifier', 'MLPRegressor']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn.neural_network\n",
    "help(sklearn.neural_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn.ensemble in sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn.ensemble\n",
      "\n",
      "FILE\n",
      "    e:\\programdata\\anaconda2\\lib\\site-packages\\sklearn\\ensemble\\__init__.py\n",
      "\n",
      "DESCRIPTION\n",
      "    The :mod:`sklearn.ensemble` module includes ensemble-based methods for\n",
      "    classification, regression and anomaly detection.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _gradient_boosting\n",
      "    bagging\n",
      "    base\n",
      "    forest\n",
      "    gradient_boosting\n",
      "    iforest\n",
      "    partial_dependence\n",
      "    setup\n",
      "    tests (package)\n",
      "    voting_classifier\n",
      "    weight_boosting\n",
      "\n",
      "CLASSES\n",
      "    abc.NewBase(sklearn.base.BaseEstimator, sklearn.base.MetaEstimatorMixin)\n",
      "        sklearn.ensemble.base.BaseEnsemble\n",
      "    sklearn.base.ClassifierMixin(__builtin__.object)\n",
      "        sklearn.ensemble.bagging.BaggingClassifier(sklearn.ensemble.bagging.BaseBagging, sklearn.base.ClassifierMixin)\n",
      "        sklearn.ensemble.gradient_boosting.GradientBoostingClassifier(sklearn.ensemble.gradient_boosting.BaseGradientBoosting, sklearn.base.ClassifierMixin)\n",
      "        sklearn.ensemble.voting_classifier.VotingClassifier(sklearn.utils.metaestimators._BaseComposition, sklearn.base.ClassifierMixin, sklearn.base.TransformerMixin)\n",
      "        sklearn.ensemble.weight_boosting.AdaBoostClassifier(sklearn.ensemble.weight_boosting.BaseWeightBoosting, sklearn.base.ClassifierMixin)\n",
      "    sklearn.base.RegressorMixin(__builtin__.object)\n",
      "        sklearn.ensemble.bagging.BaggingRegressor(sklearn.ensemble.bagging.BaseBagging, sklearn.base.RegressorMixin)\n",
      "        sklearn.ensemble.gradient_boosting.GradientBoostingRegressor(sklearn.ensemble.gradient_boosting.BaseGradientBoosting, sklearn.base.RegressorMixin)\n",
      "        sklearn.ensemble.weight_boosting.AdaBoostRegressor(sklearn.ensemble.weight_boosting.BaseWeightBoosting, sklearn.base.RegressorMixin)\n",
      "    sklearn.base.TransformerMixin(__builtin__.object)\n",
      "        sklearn.ensemble.voting_classifier.VotingClassifier(sklearn.utils.metaestimators._BaseComposition, sklearn.base.ClassifierMixin, sklearn.base.TransformerMixin)\n",
      "    sklearn.ensemble.bagging.BaseBagging(abc.NewBase)\n",
      "        sklearn.ensemble.bagging.BaggingClassifier(sklearn.ensemble.bagging.BaseBagging, sklearn.base.ClassifierMixin)\n",
      "        sklearn.ensemble.bagging.BaggingRegressor(sklearn.ensemble.bagging.BaseBagging, sklearn.base.RegressorMixin)\n",
      "        sklearn.ensemble.iforest.IsolationForest\n",
      "    sklearn.ensemble.forest.BaseForest(abc.NewBase)\n",
      "        sklearn.ensemble.forest.RandomTreesEmbedding\n",
      "    sklearn.ensemble.forest.ForestClassifier(abc.NewBase)\n",
      "        sklearn.ensemble.forest.ExtraTreesClassifier\n",
      "        sklearn.ensemble.forest.RandomForestClassifier\n",
      "    sklearn.ensemble.forest.ForestRegressor(abc.NewBase)\n",
      "        sklearn.ensemble.forest.ExtraTreesRegressor\n",
      "        sklearn.ensemble.forest.RandomForestRegressor\n",
      "    sklearn.ensemble.gradient_boosting.BaseGradientBoosting(abc.NewBase)\n",
      "        sklearn.ensemble.gradient_boosting.GradientBoostingClassifier(sklearn.ensemble.gradient_boosting.BaseGradientBoosting, sklearn.base.ClassifierMixin)\n",
      "        sklearn.ensemble.gradient_boosting.GradientBoostingRegressor(sklearn.ensemble.gradient_boosting.BaseGradientBoosting, sklearn.base.RegressorMixin)\n",
      "    sklearn.ensemble.weight_boosting.BaseWeightBoosting(abc.NewBase)\n",
      "        sklearn.ensemble.weight_boosting.AdaBoostClassifier(sklearn.ensemble.weight_boosting.BaseWeightBoosting, sklearn.base.ClassifierMixin)\n",
      "        sklearn.ensemble.weight_boosting.AdaBoostRegressor(sklearn.ensemble.weight_boosting.BaseWeightBoosting, sklearn.base.RegressorMixin)\n",
      "    sklearn.utils.metaestimators._BaseComposition(abc.NewBase)\n",
      "        sklearn.ensemble.voting_classifier.VotingClassifier(sklearn.utils.metaestimators._BaseComposition, sklearn.base.ClassifierMixin, sklearn.base.TransformerMixin)\n",
      "    \n",
      "    class AdaBoostClassifier(BaseWeightBoosting, sklearn.base.ClassifierMixin)\n",
      "     |  An AdaBoost classifier.\n",
      "     |  \n",
      "     |  An AdaBoost [1] classifier is a meta-estimator that begins by fitting a\n",
      "     |  classifier on the original dataset and then fits additional copies of the\n",
      "     |  classifier on the same dataset but where the weights of incorrectly\n",
      "     |  classified instances are adjusted such that subsequent classifiers focus\n",
      "     |  more on difficult cases.\n",
      "     |  \n",
      "     |  This class implements the algorithm known as AdaBoost-SAMME [2].\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <adaboost>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  base_estimator : object, optional (default=DecisionTreeClassifier)\n",
      "     |      The base estimator from which the boosted ensemble is built.\n",
      "     |      Support for sample weighting is required, as well as proper `classes_`\n",
      "     |      and `n_classes_` attributes.\n",
      "     |  \n",
      "     |  n_estimators : integer, optional (default=50)\n",
      "     |      The maximum number of estimators at which boosting is terminated.\n",
      "     |      In case of perfect fit, the learning procedure is stopped early.\n",
      "     |  \n",
      "     |  learning_rate : float, optional (default=1.)\n",
      "     |      Learning rate shrinks the contribution of each classifier by\n",
      "     |      ``learning_rate``. There is a trade-off between ``learning_rate`` and\n",
      "     |      ``n_estimators``.\n",
      "     |  \n",
      "     |  algorithm : {'SAMME', 'SAMME.R'}, optional (default='SAMME.R')\n",
      "     |      If 'SAMME.R' then use the SAMME.R real boosting algorithm.\n",
      "     |      ``base_estimator`` must support calculation of class probabilities.\n",
      "     |      If 'SAMME' then use the SAMME discrete boosting algorithm.\n",
      "     |      The SAMME.R algorithm typically converges faster than SAMME,\n",
      "     |      achieving a lower test error with fewer boosting iterations.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional (default=None)\n",
      "     |      If int, random_state is the seed used by the random number generator;\n",
      "     |      If RandomState instance, random_state is the random number generator;\n",
      "     |      If None, the random number generator is the RandomState instance used\n",
      "     |      by `np.random`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimators_ : list of classifiers\n",
      "     |      The collection of fitted sub-estimators.\n",
      "     |  \n",
      "     |  classes_ : array of shape = [n_classes]\n",
      "     |      The classes labels.\n",
      "     |  \n",
      "     |  n_classes_ : int\n",
      "     |      The number of classes.\n",
      "     |  \n",
      "     |  estimator_weights_ : array of floats\n",
      "     |      Weights for each estimator in the boosted ensemble.\n",
      "     |  \n",
      "     |  estimator_errors_ : array of floats\n",
      "     |      Classification error for each estimator in the boosted\n",
      "     |      ensemble.\n",
      "     |  \n",
      "     |  feature_importances_ : array of shape = [n_features]\n",
      "     |      The feature importances if supported by the ``base_estimator``.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  AdaBoostRegressor, GradientBoostingClassifier, DecisionTreeClassifier\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n",
      "     |         on-Line Learning and an Application to Boosting\", 1995.\n",
      "     |  \n",
      "     |  .. [2] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      AdaBoostClassifier\n",
      "     |      BaseWeightBoosting\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.ensemble.base.BaseEnsemble\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, base_estimator=None, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None)\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Compute the decision function of ``X``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. DOK and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : array, shape = [n_samples, k]\n",
      "     |          The decision function of the input samples. The order of\n",
      "     |          outputs is the same of that of the `classes_` attribute.\n",
      "     |          Binary classification is a special cases with ``k == 1``,\n",
      "     |          otherwise ``k==n_classes``. For binary classification,\n",
      "     |          values closer to -1 or 1 mean more like the first or second\n",
      "     |          class in ``classes_``, respectively.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Build a boosted classifier from the training set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. DOK and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      y : array-like of shape = [n_samples]\n",
      "     |          The target values (class labels).\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape = [n_samples], optional\n",
      "     |          Sample weights. If None, the sample weights are initialized to\n",
      "     |          ``1 / n_samples``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict classes for X.\n",
      "     |      \n",
      "     |      The predicted class of an input sample is computed as the weighted mean\n",
      "     |      prediction of the classifiers in the ensemble.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. DOK and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array of shape = [n_samples]\n",
      "     |          The predicted classes.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Predict class log-probabilities for X.\n",
      "     |      \n",
      "     |      The predicted class log-probabilities of an input sample is computed as\n",
      "     |      the weighted mean predicted class log-probabilities of the classifiers\n",
      "     |      in the ensemble.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. DOK and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : array of shape = [n_samples]\n",
      "     |          The class probabilities of the input samples. The order of\n",
      "     |          outputs is the same of that of the `classes_` attribute.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Predict class probabilities for X.\n",
      "     |      \n",
      "     |      The predicted class probabilities of an input sample is computed as\n",
      "     |      the weighted mean predicted class probabilities of the classifiers\n",
      "     |      in the ensemble.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. DOK and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : array of shape = [n_samples]\n",
      "     |          The class probabilities of the input samples. The order of\n",
      "     |          outputs is the same of that of the `classes_` attribute.\n",
      "     |  \n",
      "     |  staged_decision_function(self, X)\n",
      "     |      Compute decision function of ``X`` for each boosting iteration.\n",
      "     |      \n",
      "     |      This method allows monitoring (i.e. determine error on testing set)\n",
      "     |      after each boosting iteration.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. DOK and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : generator of array, shape = [n_samples, k]\n",
      "     |          The decision function of the input samples. The order of\n",
      "     |          outputs is the same of that of the `classes_` attribute.\n",
      "     |          Binary classification is a special cases with ``k == 1``,\n",
      "     |          otherwise ``k==n_classes``. For binary classification,\n",
      "     |          values closer to -1 or 1 mean more like the first or second\n",
      "     |          class in ``classes_``, respectively.\n",
      "     |  \n",
      "     |  staged_predict(self, X)\n",
      "     |      Return staged predictions for X.\n",
      "     |      \n",
      "     |      The predicted class of an input sample is computed as the weighted mean\n",
      "     |      prediction of the classifiers in the ensemble.\n",
      "     |      \n",
      "     |      This generator method yields the ensemble prediction after each\n",
      "     |      iteration of boosting and therefore allows monitoring, such as to\n",
      "     |      determine the prediction on a test set after each boost.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape = [n_samples, n_features]\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : generator of array, shape = [n_samples]\n",
      "     |          The predicted classes.\n",
      "     |  \n",
      "     |  staged_predict_proba(self, X)\n",
      "     |      Predict class probabilities for X.\n",
      "     |      \n",
      "     |      The predicted class probabilities of an input sample is computed as\n",
      "     |      the weighted mean predicted class probabilities of the classifiers\n",
      "     |      in the ensemble.\n",
      "     |      \n",
      "     |      This generator method yields the ensemble predicted class probabilities\n",
      "     |      after each iteration of boosting and therefore allows monitoring, such\n",
      "     |      as to determine the predicted class probabilities on a test set after\n",
      "     |      each boost.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. DOK and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : generator of array, shape = [n_samples]\n",
      "     |          The class probabilities of the input samples. The order of\n",
      "     |          outputs is the same of that of the `classes_` attribute.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseWeightBoosting:\n",
      "     |  \n",
      "     |  staged_score(self, X, y, sample_weight=None)\n",
      "     |      Return staged scores for X, y.\n",
      "     |      \n",
      "     |      This generator method yields the ensemble score after each iteration of\n",
      "     |      boosting and therefore allows monitoring, such as to determine the\n",
      "     |      score on a test set after each boost.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. DOK and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples]\n",
      "     |          Labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      z : float\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseWeightBoosting:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Return the feature importances (the higher, the more important the\n",
      "     |         feature).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array, shape = [n_features]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble.base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Returns the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Returns iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Returns the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "    \n",
      "    class AdaBoostRegressor(BaseWeightBoosting, sklearn.base.RegressorMixin)\n",
      "     |  An AdaBoost regressor.\n",
      "     |  \n",
      "     |  An AdaBoost [1] regressor is a meta-estimator that begins by fitting a\n",
      "     |  regressor on the original dataset and then fits additional copies of the\n",
      "     |  regressor on the same dataset but where the weights of instances are\n",
      "     |  adjusted according to the error of the current prediction. As such,\n",
      "     |  subsequent regressors focus more on difficult cases.\n",
      "     |  \n",
      "     |  This class implements the algorithm known as AdaBoost.R2 [2].\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <adaboost>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  base_estimator : object, optional (default=DecisionTreeRegressor)\n",
      "     |      The base estimator from which the boosted ensemble is built.\n",
      "     |      Support for sample weighting is required.\n",
      "     |  \n",
      "     |  n_estimators : integer, optional (default=50)\n",
      "     |      The maximum number of estimators at which boosting is terminated.\n",
      "     |      In case of perfect fit, the learning procedure is stopped early.\n",
      "     |  \n",
      "     |  learning_rate : float, optional (default=1.)\n",
      "     |      Learning rate shrinks the contribution of each regressor by\n",
      "     |      ``learning_rate``. There is a trade-off between ``learning_rate`` and\n",
      "     |      ``n_estimators``.\n",
      "     |  \n",
      "     |  loss : {'linear', 'square', 'exponential'}, optional (default='linear')\n",
      "     |      The loss function to use when updating the weights after each\n",
      "     |      boosting iteration.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional (default=None)\n",
      "     |      If int, random_state is the seed used by the random number generator;\n",
      "     |      If RandomState instance, random_state is the random number generator;\n",
      "     |      If None, the random number generator is the RandomState instance used\n",
      "     |      by `np.random`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimators_ : list of classifiers\n",
      "     |      The collection of fitted sub-estimators.\n",
      "     |  \n",
      "     |  estimator_weights_ : array of floats\n",
      "     |      Weights for each estimator in the boosted ensemble.\n",
      "     |  \n",
      "     |  estimator_errors_ : array of floats\n",
      "     |      Regression error for each estimator in the boosted ensemble.\n",
      "     |  \n",
      "     |  feature_importances_ : array of shape = [n_features]\n",
      "     |      The feature importances if supported by the ``base_estimator``.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  AdaBoostClassifier, GradientBoostingRegressor, DecisionTreeRegressor\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n",
      "     |         on-Line Learning and an Application to Boosting\", 1995.\n",
      "     |  \n",
      "     |  .. [2] H. Drucker, \"Improving Regressors using Boosting Techniques\", 1997.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      AdaBoostRegressor\n",
      "     |      BaseWeightBoosting\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.ensemble.base.BaseEnsemble\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, base_estimator=None, n_estimators=50, learning_rate=1.0, loss='linear', random_state=None)\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Build a boosted regressor from the training set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. DOK and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      y : array-like of shape = [n_samples]\n",
      "     |          The target values (real numbers).\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape = [n_samples], optional\n",
      "     |          Sample weights. If None, the sample weights are initialized to\n",
      "     |          1 / n_samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict regression value for X.\n",
      "     |      \n",
      "     |      The predicted regression value of an input sample is computed\n",
      "     |      as the weighted median prediction of the classifiers in the ensemble.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. DOK and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array of shape = [n_samples]\n",
      "     |          The predicted regression values.\n",
      "     |  \n",
      "     |  staged_predict(self, X)\n",
      "     |      Return staged predictions for X.\n",
      "     |      \n",
      "     |      The predicted regression value of an input sample is computed\n",
      "     |      as the weighted median prediction of the classifiers in the ensemble.\n",
      "     |      \n",
      "     |      This generator method yields the ensemble prediction after each\n",
      "     |      iteration of boosting and therefore allows monitoring, such as to\n",
      "     |      determine the prediction on a test set after each boost.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. DOK and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : generator of array, shape = [n_samples]\n",
      "     |          The predicted regression values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseWeightBoosting:\n",
      "     |  \n",
      "     |  staged_score(self, X, y, sample_weight=None)\n",
      "     |      Return staged scores for X, y.\n",
      "     |      \n",
      "     |      This generator method yields the ensemble score after each iteration of\n",
      "     |      boosting and therefore allows monitoring, such as to determine the\n",
      "     |      score on a test set after each boost.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. DOK and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples]\n",
      "     |          Labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      z : float\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseWeightBoosting:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Return the feature importances (the higher, the more important the\n",
      "     |         feature).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array, shape = [n_features]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble.base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Returns the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Returns iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Returns the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class BaggingClassifier(BaseBagging, sklearn.base.ClassifierMixin)\n",
      "     |  A Bagging classifier.\n",
      "     |  \n",
      "     |  A Bagging classifier is an ensemble meta-estimator that fits base\n",
      "     |  classifiers each on random subsets of the original dataset and then\n",
      "     |  aggregate their individual predictions (either by voting or by averaging)\n",
      "     |  to form a final prediction. Such a meta-estimator can typically be used as\n",
      "     |  a way to reduce the variance of a black-box estimator (e.g., a decision\n",
      "     |  tree), by introducing randomization into its construction procedure and\n",
      "     |  then making an ensemble out of it.\n",
      "     |  \n",
      "     |  This algorithm encompasses several works from the literature. When random\n",
      "     |  subsets of the dataset are drawn as random subsets of the samples, then\n",
      "     |  this algorithm is known as Pasting [1]_. If samples are drawn with\n",
      "     |  replacement, then the method is known as Bagging [2]_. When random subsets\n",
      "     |  of the dataset are drawn as random subsets of the features, then the method\n",
      "     |  is known as Random Subspaces [3]_. Finally, when base estimators are built\n",
      "     |  on subsets of both samples and features, then the method is known as\n",
      "     |  Random Patches [4]_.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <bagging>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  base_estimator : object or None, optional (default=None)\n",
      "     |      The base estimator to fit on random subsets of the dataset.\n",
      "     |      If None, then the base estimator is a decision tree.\n",
      "     |  \n",
      "     |  n_estimators : int, optional (default=10)\n",
      "     |      The number of base estimators in the ensemble.\n",
      "     |  \n",
      "     |  max_samples : int or float, optional (default=1.0)\n",
      "     |      The number of samples to draw from X to train each base estimator.\n",
      "     |          - If int, then draw `max_samples` samples.\n",
      "     |          - If float, then draw `max_samples * X.shape[0]` samples.\n",
      "     |  \n",
      "     |  max_features : int or float, optional (default=1.0)\n",
      "     |      The number of features to draw from X to train each base estimator.\n",
      "     |          - If int, then draw `max_features` features.\n",
      "     |          - If float, then draw `max_features * X.shape[1]` features.\n",
      "     |  \n",
      "     |  bootstrap : boolean, optional (default=True)\n",
      "     |      Whether samples are drawn with replacement.\n",
      "     |  \n",
      "     |  bootstrap_features : boolean, optional (default=False)\n",
      "     |      Whether features are drawn with replacement.\n",
      "     |  \n",
      "     |  oob_score : bool\n",
      "     |      Whether to use out-of-bag samples to estimate\n",
      "     |      the generalization error.\n",
      "     |  \n",
      "     |  warm_start : bool, optional (default=False)\n",
      "     |      When set to True, reuse the solution of the previous call to fit\n",
      "     |      and add more estimators to the ensemble, otherwise, just fit\n",
      "     |      a whole new ensemble.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         *warm_start* constructor parameter.\n",
      "     |  \n",
      "     |  n_jobs : int, optional (default=1)\n",
      "     |      The number of jobs to run in parallel for both `fit` and `predict`.\n",
      "     |      If -1, then the number of jobs is set to the number of cores.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional (default=None)\n",
      "     |      If int, random_state is the seed used by the random number generator;\n",
      "     |      If RandomState instance, random_state is the random number generator;\n",
      "     |      If None, the random number generator is the RandomState instance used\n",
      "     |      by `np.random`.\n",
      "     |  \n",
      "     |  verbose : int, optional (default=0)\n",
      "     |      Controls the verbosity of the building process.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  base_estimator_ : estimator\n",
      "     |      The base estimator from which the ensemble is grown.\n",
      "     |  \n",
      "     |  estimators_ : list of estimators\n",
      "     |      The collection of fitted base estimators.\n",
      "     |  \n",
      "     |  estimators_samples_ : list of arrays\n",
      "     |      The subset of drawn samples (i.e., the in-bag samples) for each base\n",
      "     |      estimator. Each subset is defined by a boolean mask.\n",
      "     |  \n",
      "     |  estimators_features_ : list of arrays\n",
      "     |      The subset of drawn features for each base estimator.\n",
      "     |  \n",
      "     |  classes_ : array of shape = [n_classes]\n",
      "     |      The classes labels.\n",
      "     |  \n",
      "     |  n_classes_ : int or list\n",
      "     |      The number of classes.\n",
      "     |  \n",
      "     |  oob_score_ : float\n",
      "     |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      "     |  \n",
      "     |  oob_decision_function_ : array of shape = [n_samples, n_classes]\n",
      "     |      Decision function computed with out-of-bag estimate on the training\n",
      "     |      set. If n_estimators is small it might be possible that a data point\n",
      "     |      was never left out during the bootstrap. In this case,\n",
      "     |      `oob_decision_function_` might contain NaN.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  .. [1] L. Breiman, \"Pasting small votes for classification in large\n",
      "     |         databases and on-line\", Machine Learning, 36(1), 85-103, 1999.\n",
      "     |  \n",
      "     |  .. [2] L. Breiman, \"Bagging predictors\", Machine Learning, 24(2), 123-140,\n",
      "     |         1996.\n",
      "     |  \n",
      "     |  .. [3] T. Ho, \"The random subspace method for constructing decision\n",
      "     |         forests\", Pattern Analysis and Machine Intelligence, 20(8), 832-844,\n",
      "     |         1998.\n",
      "     |  \n",
      "     |  .. [4] G. Louppe and P. Geurts, \"Ensembles on Random Patches\", Machine\n",
      "     |         Learning and Knowledge Discovery in Databases, 346-361, 2012.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BaggingClassifier\n",
      "     |      BaseBagging\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.ensemble.base.BaseEnsemble\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, base_estimator=None, n_estimators=10, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=1, random_state=None, verbose=0)\n",
      "     |  \n",
      "     |  decision_function(*args, **kwargs)\n",
      "     |      Average of the decision functions of the base classifiers.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Sparse matrices are accepted only if\n",
      "     |          they are supported by the base estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : array, shape = [n_samples, k]\n",
      "     |          The decision function of the input samples. The columns correspond\n",
      "     |          to the classes in sorted order, as they appear in the attribute\n",
      "     |          ``classes_``. Regression and binary classification are special\n",
      "     |          cases with ``k == 1``, otherwise ``k==n_classes``.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class for X.\n",
      "     |      \n",
      "     |      The predicted class of an input sample is computed as the class with\n",
      "     |      the highest mean predicted probability. If base estimators do not\n",
      "     |      implement a ``predict_proba`` method, then it resorts to voting.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Sparse matrices are accepted only if\n",
      "     |          they are supported by the base estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array of shape = [n_samples]\n",
      "     |          The predicted classes.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Predict class log-probabilities for X.\n",
      "     |      \n",
      "     |      The predicted class log-probabilities of an input sample is computed as\n",
      "     |      the log of the mean predicted class probabilities of the base\n",
      "     |      estimators in the ensemble.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Sparse matrices are accepted only if\n",
      "     |          they are supported by the base estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : array of shape = [n_samples, n_classes]\n",
      "     |          The class log-probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute `classes_`.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Predict class probabilities for X.\n",
      "     |      \n",
      "     |      The predicted class probabilities of an input sample is computed as\n",
      "     |      the mean predicted class probabilities of the base estimators in the\n",
      "     |      ensemble. If base estimators do not implement a ``predict_proba``\n",
      "     |      method, then it resorts to voting and the predicted class probabilities\n",
      "     |      of an input sample represents the proportion of estimators predicting\n",
      "     |      each class.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Sparse matrices are accepted only if\n",
      "     |          they are supported by the base estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : array of shape = [n_samples, n_classes]\n",
      "     |          The class probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute `classes_`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseBagging:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Build a Bagging ensemble of estimators from the training\n",
      "     |         set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Sparse matrices are accepted only if\n",
      "     |          they are supported by the base estimator.\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples]\n",
      "     |          The target values (class labels in classification, real numbers in\n",
      "     |          regression).\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples] or None\n",
      "     |          Sample weights. If None, then samples are equally weighted.\n",
      "     |          Note that this is supported only if the base estimator supports\n",
      "     |          sample weighting.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseBagging:\n",
      "     |  \n",
      "     |  estimators_samples_\n",
      "     |      The subset of drawn samples for each base estimator.\n",
      "     |      \n",
      "     |      Returns a dynamically generated list of boolean masks identifying\n",
      "     |      the samples used for fitting each member of the ensemble, i.e.,\n",
      "     |      the in-bag samples.\n",
      "     |      \n",
      "     |      Note: the list is re-created at each call to the property in order\n",
      "     |      to reduce the object memory footprint by not storing the sampling\n",
      "     |      data. Thus fetching the property may be slower than expected.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble.base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Returns the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Returns iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Returns the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "    \n",
      "    class BaggingRegressor(BaseBagging, sklearn.base.RegressorMixin)\n",
      "     |  A Bagging regressor.\n",
      "     |  \n",
      "     |  A Bagging regressor is an ensemble meta-estimator that fits base\n",
      "     |  regressors each on random subsets of the original dataset and then\n",
      "     |  aggregate their individual predictions (either by voting or by averaging)\n",
      "     |  to form a final prediction. Such a meta-estimator can typically be used as\n",
      "     |  a way to reduce the variance of a black-box estimator (e.g., a decision\n",
      "     |  tree), by introducing randomization into its construction procedure and\n",
      "     |  then making an ensemble out of it.\n",
      "     |  \n",
      "     |  This algorithm encompasses several works from the literature. When random\n",
      "     |  subsets of the dataset are drawn as random subsets of the samples, then\n",
      "     |  this algorithm is known as Pasting [1]_. If samples are drawn with\n",
      "     |  replacement, then the method is known as Bagging [2]_. When random subsets\n",
      "     |  of the dataset are drawn as random subsets of the features, then the method\n",
      "     |  is known as Random Subspaces [3]_. Finally, when base estimators are built\n",
      "     |  on subsets of both samples and features, then the method is known as\n",
      "     |  Random Patches [4]_.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <bagging>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  base_estimator : object or None, optional (default=None)\n",
      "     |      The base estimator to fit on random subsets of the dataset.\n",
      "     |      If None, then the base estimator is a decision tree.\n",
      "     |  \n",
      "     |  n_estimators : int, optional (default=10)\n",
      "     |      The number of base estimators in the ensemble.\n",
      "     |  \n",
      "     |  max_samples : int or float, optional (default=1.0)\n",
      "     |      The number of samples to draw from X to train each base estimator.\n",
      "     |          - If int, then draw `max_samples` samples.\n",
      "     |          - If float, then draw `max_samples * X.shape[0]` samples.\n",
      "     |  \n",
      "     |  max_features : int or float, optional (default=1.0)\n",
      "     |      The number of features to draw from X to train each base estimator.\n",
      "     |          - If int, then draw `max_features` features.\n",
      "     |          - If float, then draw `max_features * X.shape[1]` features.\n",
      "     |  \n",
      "     |  bootstrap : boolean, optional (default=True)\n",
      "     |      Whether samples are drawn with replacement.\n",
      "     |  \n",
      "     |  bootstrap_features : boolean, optional (default=False)\n",
      "     |      Whether features are drawn with replacement.\n",
      "     |  \n",
      "     |  oob_score : bool\n",
      "     |      Whether to use out-of-bag samples to estimate\n",
      "     |      the generalization error.\n",
      "     |  \n",
      "     |  warm_start : bool, optional (default=False)\n",
      "     |      When set to True, reuse the solution of the previous call to fit\n",
      "     |      and add more estimators to the ensemble, otherwise, just fit\n",
      "     |      a whole new ensemble.\n",
      "     |  \n",
      "     |  n_jobs : int, optional (default=1)\n",
      "     |      The number of jobs to run in parallel for both `fit` and `predict`.\n",
      "     |      If -1, then the number of jobs is set to the number of cores.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional (default=None)\n",
      "     |      If int, random_state is the seed used by the random number generator;\n",
      "     |      If RandomState instance, random_state is the random number generator;\n",
      "     |      If None, the random number generator is the RandomState instance used\n",
      "     |      by `np.random`.\n",
      "     |  \n",
      "     |  verbose : int, optional (default=0)\n",
      "     |      Controls the verbosity of the building process.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimators_ : list of estimators\n",
      "     |      The collection of fitted sub-estimators.\n",
      "     |  \n",
      "     |  estimators_samples_ : list of arrays\n",
      "     |      The subset of drawn samples (i.e., the in-bag samples) for each base\n",
      "     |      estimator. Each subset is defined by a boolean mask.\n",
      "     |  \n",
      "     |  estimators_features_ : list of arrays\n",
      "     |      The subset of drawn features for each base estimator.\n",
      "     |  \n",
      "     |  oob_score_ : float\n",
      "     |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      "     |  \n",
      "     |  oob_prediction_ : array of shape = [n_samples]\n",
      "     |      Prediction computed with out-of-bag estimate on the training\n",
      "     |      set. If n_estimators is small it might be possible that a data point\n",
      "     |      was never left out during the bootstrap. In this case,\n",
      "     |      `oob_prediction_` might contain NaN.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  .. [1] L. Breiman, \"Pasting small votes for classification in large\n",
      "     |         databases and on-line\", Machine Learning, 36(1), 85-103, 1999.\n",
      "     |  \n",
      "     |  .. [2] L. Breiman, \"Bagging predictors\", Machine Learning, 24(2), 123-140,\n",
      "     |         1996.\n",
      "     |  \n",
      "     |  .. [3] T. Ho, \"The random subspace method for constructing decision\n",
      "     |         forests\", Pattern Analysis and Machine Intelligence, 20(8), 832-844,\n",
      "     |         1998.\n",
      "     |  \n",
      "     |  .. [4] G. Louppe and P. Geurts, \"Ensembles on Random Patches\", Machine\n",
      "     |         Learning and Knowledge Discovery in Databases, 346-361, 2012.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BaggingRegressor\n",
      "     |      BaseBagging\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.ensemble.base.BaseEnsemble\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, base_estimator=None, n_estimators=10, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=1, random_state=None, verbose=0)\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict regression target for X.\n",
      "     |      \n",
      "     |      The predicted regression target of an input sample is computed as the\n",
      "     |      mean predicted regression targets of the estimators in the ensemble.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Sparse matrices are accepted only if\n",
      "     |          they are supported by the base estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array of shape = [n_samples]\n",
      "     |          The predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseBagging:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Build a Bagging ensemble of estimators from the training\n",
      "     |         set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Sparse matrices are accepted only if\n",
      "     |          they are supported by the base estimator.\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples]\n",
      "     |          The target values (class labels in classification, real numbers in\n",
      "     |          regression).\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples] or None\n",
      "     |          Sample weights. If None, then samples are equally weighted.\n",
      "     |          Note that this is supported only if the base estimator supports\n",
      "     |          sample weighting.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseBagging:\n",
      "     |  \n",
      "     |  estimators_samples_\n",
      "     |      The subset of drawn samples for each base estimator.\n",
      "     |      \n",
      "     |      Returns a dynamically generated list of boolean masks identifying\n",
      "     |      the samples used for fitting each member of the ensemble, i.e.,\n",
      "     |      the in-bag samples.\n",
      "     |      \n",
      "     |      Note: the list is re-created at each call to the property in order\n",
      "     |      to reduce the object memory footprint by not storing the sampling\n",
      "     |      data. Thus fetching the property may be slower than expected.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble.base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Returns the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Returns iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Returns the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class BaseEnsemble(abc.NewBase)\n",
      "     |  Base class for all ensemble classes.\n",
      "     |  \n",
      "     |  Warning: This class should not be used directly. Use derived classes\n",
      "     |  instead.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  base_estimator : object, optional (default=None)\n",
      "     |      The base estimator from which the ensemble is built.\n",
      "     |  \n",
      "     |  n_estimators : integer\n",
      "     |      The number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  estimator_params : list of strings\n",
      "     |      The list of attributes to use as parameters when instantiating a\n",
      "     |      new base estimator. If none are given, default parameters are used.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  base_estimator_ : estimator\n",
      "     |      The base estimator from which the ensemble is grown.\n",
      "     |  \n",
      "     |  estimators_ : list of estimators\n",
      "     |      The collection of fitted base estimators.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BaseEnsemble\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Returns the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __init__(self, base_estimator, n_estimators=10, estimator_params=())\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Returns iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Returns the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset(['__init__'])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class ExtraTreesClassifier(ForestClassifier)\n",
      "     |  An extra-trees classifier.\n",
      "     |  \n",
      "     |  This class implements a meta estimator that fits a number of\n",
      "     |  randomized decision trees (a.k.a. extra-trees) on various sub-samples\n",
      "     |  of the dataset and use averaging to improve the predictive accuracy\n",
      "     |  and control over-fitting.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <forest>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_estimators : integer, optional (default=10)\n",
      "     |      The number of trees in the forest.\n",
      "     |  \n",
      "     |  criterion : string, optional (default=\"gini\")\n",
      "     |      The function to measure the quality of a split. Supported criteria are\n",
      "     |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
      "     |  \n",
      "     |  max_features : int, float, string or None, optional (default=\"auto\")\n",
      "     |      The number of features to consider when looking for the best split:\n",
      "     |  \n",
      "     |      - If int, then consider `max_features` features at each split.\n",
      "     |      - If float, then `max_features` is a percentage and\n",
      "     |        `int(max_features * n_features)` features are considered at each\n",
      "     |        split.\n",
      "     |      - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      "     |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "     |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      "     |      - If None, then `max_features=n_features`.\n",
      "     |  \n",
      "     |      Note: the search for a split does not stop until at least one\n",
      "     |      valid partition of the node samples is found, even if it requires to\n",
      "     |      effectively inspect more than ``max_features`` features.\n",
      "     |  \n",
      "     |  max_depth : integer or None, optional (default=None)\n",
      "     |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      "     |      all leaves are pure or until all leaves contain less than\n",
      "     |      min_samples_split samples.\n",
      "     |  \n",
      "     |  min_samples_split : int, float, optional (default=2)\n",
      "     |      The minimum number of samples required to split an internal node:\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_split` as the minimum number.\n",
      "     |      - If float, then `min_samples_split` is a percentage and\n",
      "     |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      "     |        number of samples for each split.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for percentages.\n",
      "     |  \n",
      "     |  min_samples_leaf : int, float, optional (default=1)\n",
      "     |      The minimum number of samples required to be at a leaf node:\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "     |      - If float, then `min_samples_leaf` is a percentage and\n",
      "     |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "     |        number of samples for each node.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for percentages.\n",
      "     |  \n",
      "     |  min_weight_fraction_leaf : float, optional (default=0.)\n",
      "     |      The minimum weighted fraction of the sum total of weights (of all\n",
      "     |      the input samples) required to be at a leaf node. Samples have\n",
      "     |      equal weight when sample_weight is not provided.\n",
      "     |  \n",
      "     |  max_leaf_nodes : int or None, optional (default=None)\n",
      "     |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "     |      Best nodes are defined as relative reduction in impurity.\n",
      "     |      If None then unlimited number of leaf nodes.\n",
      "     |  \n",
      "     |  min_impurity_split : float,\n",
      "     |      Threshold for early stopping in tree growth. A node will split\n",
      "     |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      "     |  \n",
      "     |      .. deprecated:: 0.19\n",
      "     |         ``min_impurity_split`` has been deprecated in favor of\n",
      "     |         ``min_impurity_decrease`` in 0.19 and will be removed in 0.21.\n",
      "     |         Use ``min_impurity_decrease`` instead.\n",
      "     |  \n",
      "     |  min_impurity_decrease : float, optional (default=0.)\n",
      "     |      A node will be split if this split induces a decrease of the impurity\n",
      "     |      greater than or equal to this value.\n",
      "     |  \n",
      "     |      The weighted impurity decrease equation is the following::\n",
      "     |  \n",
      "     |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "     |                              - N_t_L / N_t * left_impurity)\n",
      "     |  \n",
      "     |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "     |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "     |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "     |  \n",
      "     |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "     |      if ``sample_weight`` is passed.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  bootstrap : boolean, optional (default=False)\n",
      "     |      Whether bootstrap samples are used when building trees.\n",
      "     |  \n",
      "     |  oob_score : bool, optional (default=False)\n",
      "     |      Whether to use out-of-bag samples to estimate\n",
      "     |      the generalization accuracy.\n",
      "     |  \n",
      "     |  n_jobs : integer, optional (default=1)\n",
      "     |      The number of jobs to run in parallel for both `fit` and `predict`.\n",
      "     |      If -1, then the number of jobs is set to the number of cores.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional (default=None)\n",
      "     |      If int, random_state is the seed used by the random number generator;\n",
      "     |      If RandomState instance, random_state is the random number generator;\n",
      "     |      If None, the random number generator is the RandomState instance used\n",
      "     |      by `np.random`.\n",
      "     |  \n",
      "     |  verbose : int, optional (default=0)\n",
      "     |      Controls the verbosity of the tree building process.\n",
      "     |  \n",
      "     |  warm_start : bool, optional (default=False)\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit\n",
      "     |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      "     |      new forest.\n",
      "     |  \n",
      "     |  class_weight : dict, list of dicts, \"balanced\", \"balanced_subsample\" or None, optional (default=None)\n",
      "     |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      "     |      If not given, all classes are supposed to have weight one. For\n",
      "     |      multi-output problems, a list of dicts can be provided in the same\n",
      "     |      order as the columns of y.\n",
      "     |  \n",
      "     |      Note that for multioutput (including multilabel) weights should be\n",
      "     |      defined for each class of every column in its own dict. For example,\n",
      "     |      for four-class multilabel classification weights should be\n",
      "     |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
      "     |      [{1:1}, {2:5}, {3:1}, {4:1}].\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      "     |  \n",
      "     |      The \"balanced_subsample\" mode is the same as \"balanced\" except that weights are\n",
      "     |      computed based on the bootstrap sample for every tree grown.\n",
      "     |  \n",
      "     |      For multi-output, the weights of each column of y will be multiplied.\n",
      "     |  \n",
      "     |      Note that these weights will be multiplied with sample_weight (passed\n",
      "     |      through the fit method) if sample_weight is specified.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimators_ : list of DecisionTreeClassifier\n",
      "     |      The collection of fitted sub-estimators.\n",
      "     |  \n",
      "     |  classes_ : array of shape = [n_classes] or a list of such arrays\n",
      "     |      The classes labels (single output problem), or a list of arrays of\n",
      "     |      class labels (multi-output problem).\n",
      "     |  \n",
      "     |  n_classes_ : int or list\n",
      "     |      The number of classes (single output problem), or a list containing the\n",
      "     |      number of classes for each output (multi-output problem).\n",
      "     |  \n",
      "     |  feature_importances_ : array of shape = [n_features]\n",
      "     |      The feature importances (the higher, the more important the feature).\n",
      "     |  \n",
      "     |  n_features_ : int\n",
      "     |      The number of features when ``fit`` is performed.\n",
      "     |  \n",
      "     |  n_outputs_ : int\n",
      "     |      The number of outputs when ``fit`` is performed.\n",
      "     |  \n",
      "     |  oob_score_ : float\n",
      "     |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      "     |  \n",
      "     |  oob_decision_function_ : array of shape = [n_samples, n_classes]\n",
      "     |      Decision function computed with out-of-bag estimate on the training\n",
      "     |      set. If n_estimators is small it might be possible that a data point\n",
      "     |      was never left out during the bootstrap. In this case,\n",
      "     |      `oob_decision_function_` might contain NaN.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The default values for the parameters controlling the size of the trees\n",
      "     |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      "     |  unpruned trees which can potentially be very large on some data sets. To\n",
      "     |  reduce memory consumption, the complexity and size of the trees should be\n",
      "     |  controlled by setting those parameter values.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n",
      "     |         Machine Learning, 63(1), 3-42, 2006.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  sklearn.tree.ExtraTreeClassifier : Base classifier for this ensemble.\n",
      "     |  RandomForestClassifier : Ensemble Classifier based on trees with optimal\n",
      "     |      splits.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ExtraTreesClassifier\n",
      "     |      ForestClassifier\n",
      "     |      abc.NewBase\n",
      "     |      BaseForest\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.ensemble.base.BaseEnsemble\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_estimators=10, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=False, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False, class_weight=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from ForestClassifier:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class for X.\n",
      "     |      \n",
      "     |      The predicted class of an input sample is a vote by the trees in\n",
      "     |      the forest, weighted by their probability estimates. That is,\n",
      "     |      the predicted class is the one with highest mean probability\n",
      "     |      estimate across the trees.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array of shape = [n_samples] or [n_samples, n_outputs]\n",
      "     |          The predicted classes.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Predict class log-probabilities for X.\n",
      "     |      \n",
      "     |      The predicted class log-probabilities of an input sample is computed as\n",
      "     |      the log of the mean predicted class probabilities of the trees in the\n",
      "     |      forest.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : array of shape = [n_samples, n_classes], or a list of n_outputs\n",
      "     |          such arrays if n_outputs > 1.\n",
      "     |          The class probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute `classes_`.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Predict class probabilities for X.\n",
      "     |      \n",
      "     |      The predicted class probabilities of an input sample are computed as\n",
      "     |      the mean predicted class probabilities of the trees in the forest. The\n",
      "     |      class probability of a single tree is the fraction of samples of the same\n",
      "     |      class in a leaf.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : array of shape = [n_samples, n_classes], or a list of n_outputs\n",
      "     |          such arrays if n_outputs > 1.\n",
      "     |          The class probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute `classes_`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseForest:\n",
      "     |  \n",
      "     |  apply(self, X)\n",
      "     |      Apply trees in the forest to X, return leaf indices.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape = [n_samples, n_estimators]\n",
      "     |          For each datapoint x in X and for each tree in the forest,\n",
      "     |          return the index of the leaf x ends up in.\n",
      "     |  \n",
      "     |  decision_path(self, X)\n",
      "     |      Return the decision path in the forest\n",
      "     |      \n",
      "     |      .. versionadded:: 0.18\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      indicator : sparse csr array, shape = [n_samples, n_nodes]\n",
      "     |          Return a node indicator matrix where non zero elements\n",
      "     |          indicates that the samples goes through the nodes.\n",
      "     |      \n",
      "     |      n_nodes_ptr : array of size (n_estimators + 1, )\n",
      "     |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      "     |          gives the indicator value for the i-th estimator.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Build a forest of trees from the training set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csc_matrix``.\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples] or [n_samples, n_outputs]\n",
      "     |          The target values (class labels in classification, real numbers in\n",
      "     |          regression).\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples] or None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node. In the case of\n",
      "     |          classification, splits are also ignored if they would result in any\n",
      "     |          single class carrying a negative weight in either child node.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseForest:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Return the feature importances (the higher, the more important the\n",
      "     |         feature).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array, shape = [n_features]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble.base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Returns the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Returns iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Returns the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "    \n",
      "    class ExtraTreesRegressor(ForestRegressor)\n",
      "     |  An extra-trees regressor.\n",
      "     |  \n",
      "     |  This class implements a meta estimator that fits a number of\n",
      "     |  randomized decision trees (a.k.a. extra-trees) on various sub-samples\n",
      "     |  of the dataset and use averaging to improve the predictive accuracy\n",
      "     |  and control over-fitting.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <forest>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_estimators : integer, optional (default=10)\n",
      "     |      The number of trees in the forest.\n",
      "     |  \n",
      "     |  criterion : string, optional (default=\"mse\")\n",
      "     |      The function to measure the quality of a split. Supported criteria\n",
      "     |      are \"mse\" for the mean squared error, which is equal to variance\n",
      "     |      reduction as feature selection criterion, and \"mae\" for the mean\n",
      "     |      absolute error.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |         Mean Absolute Error (MAE) criterion.\n",
      "     |  \n",
      "     |  max_features : int, float, string or None, optional (default=\"auto\")\n",
      "     |      The number of features to consider when looking for the best split:\n",
      "     |  \n",
      "     |      - If int, then consider `max_features` features at each split.\n",
      "     |      - If float, then `max_features` is a percentage and\n",
      "     |        `int(max_features * n_features)` features are considered at each\n",
      "     |        split.\n",
      "     |      - If \"auto\", then `max_features=n_features`.\n",
      "     |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "     |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      "     |      - If None, then `max_features=n_features`.\n",
      "     |  \n",
      "     |      Note: the search for a split does not stop until at least one\n",
      "     |      valid partition of the node samples is found, even if it requires to\n",
      "     |      effectively inspect more than ``max_features`` features.\n",
      "     |  \n",
      "     |  max_depth : integer or None, optional (default=None)\n",
      "     |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      "     |      all leaves are pure or until all leaves contain less than\n",
      "     |      min_samples_split samples.\n",
      "     |  \n",
      "     |  min_samples_split : int, float, optional (default=2)\n",
      "     |      The minimum number of samples required to split an internal node:\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_split` as the minimum number.\n",
      "     |      - If float, then `min_samples_split` is a percentage and\n",
      "     |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      "     |        number of samples for each split.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for percentages.\n",
      "     |  \n",
      "     |  min_samples_leaf : int, float, optional (default=1)\n",
      "     |      The minimum number of samples required to be at a leaf node:\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "     |      - If float, then `min_samples_leaf` is a percentage and\n",
      "     |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "     |        number of samples for each node.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for percentages.\n",
      "     |  \n",
      "     |  min_weight_fraction_leaf : float, optional (default=0.)\n",
      "     |      The minimum weighted fraction of the sum total of weights (of all\n",
      "     |      the input samples) required to be at a leaf node. Samples have\n",
      "     |      equal weight when sample_weight is not provided.\n",
      "     |  \n",
      "     |  max_leaf_nodes : int or None, optional (default=None)\n",
      "     |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "     |      Best nodes are defined as relative reduction in impurity.\n",
      "     |      If None then unlimited number of leaf nodes.\n",
      "     |  \n",
      "     |  min_impurity_split : float,\n",
      "     |      Threshold for early stopping in tree growth. A node will split\n",
      "     |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      "     |  \n",
      "     |      .. deprecated:: 0.19\n",
      "     |         ``min_impurity_split`` has been deprecated in favor of\n",
      "     |         ``min_impurity_decrease`` in 0.19 and will be removed in 0.21.\n",
      "     |         Use ``min_impurity_decrease`` instead.\n",
      "     |  \n",
      "     |  min_impurity_decrease : float, optional (default=0.)\n",
      "     |      A node will be split if this split induces a decrease of the impurity\n",
      "     |      greater than or equal to this value.\n",
      "     |  \n",
      "     |      The weighted impurity decrease equation is the following::\n",
      "     |  \n",
      "     |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "     |                              - N_t_L / N_t * left_impurity)\n",
      "     |  \n",
      "     |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "     |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "     |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "     |  \n",
      "     |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "     |      if ``sample_weight`` is passed.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  bootstrap : boolean, optional (default=False)\n",
      "     |      Whether bootstrap samples are used when building trees.\n",
      "     |  \n",
      "     |  oob_score : bool, optional (default=False)\n",
      "     |      Whether to use out-of-bag samples to estimate the R^2 on unseen data.\n",
      "     |  \n",
      "     |  n_jobs : integer, optional (default=1)\n",
      "     |      The number of jobs to run in parallel for both `fit` and `predict`.\n",
      "     |      If -1, then the number of jobs is set to the number of cores.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional (default=None)\n",
      "     |      If int, random_state is the seed used by the random number generator;\n",
      "     |      If RandomState instance, random_state is the random number generator;\n",
      "     |      If None, the random number generator is the RandomState instance used\n",
      "     |      by `np.random`.\n",
      "     |  \n",
      "     |  verbose : int, optional (default=0)\n",
      "     |      Controls the verbosity of the tree building process.\n",
      "     |  \n",
      "     |  warm_start : bool, optional (default=False)\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit\n",
      "     |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      "     |      new forest.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimators_ : list of DecisionTreeRegressor\n",
      "     |      The collection of fitted sub-estimators.\n",
      "     |  \n",
      "     |  feature_importances_ : array of shape = [n_features]\n",
      "     |      The feature importances (the higher, the more important the feature).\n",
      "     |  \n",
      "     |  n_features_ : int\n",
      "     |      The number of features.\n",
      "     |  \n",
      "     |  n_outputs_ : int\n",
      "     |      The number of outputs.\n",
      "     |  \n",
      "     |  oob_score_ : float\n",
      "     |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      "     |  \n",
      "     |  oob_prediction_ : array of shape = [n_samples]\n",
      "     |      Prediction computed with out-of-bag estimate on the training set.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The default values for the parameters controlling the size of the trees\n",
      "     |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      "     |  unpruned trees which can potentially be very large on some data sets. To\n",
      "     |  reduce memory consumption, the complexity and size of the trees should be\n",
      "     |  controlled by setting those parameter values.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n",
      "     |         Machine Learning, 63(1), 3-42, 2006.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  sklearn.tree.ExtraTreeRegressor: Base estimator for this ensemble.\n",
      "     |  RandomForestRegressor: Ensemble regressor using trees with optimal splits.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ExtraTreesRegressor\n",
      "     |      ForestRegressor\n",
      "     |      abc.NewBase\n",
      "     |      BaseForest\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.ensemble.base.BaseEnsemble\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_estimators=10, criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=False, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from ForestRegressor:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict regression target for X.\n",
      "     |      \n",
      "     |      The predicted regression target of an input sample is computed as the\n",
      "     |      mean predicted regression targets of the trees in the forest.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array of shape = [n_samples] or [n_samples, n_outputs]\n",
      "     |          The predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseForest:\n",
      "     |  \n",
      "     |  apply(self, X)\n",
      "     |      Apply trees in the forest to X, return leaf indices.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape = [n_samples, n_estimators]\n",
      "     |          For each datapoint x in X and for each tree in the forest,\n",
      "     |          return the index of the leaf x ends up in.\n",
      "     |  \n",
      "     |  decision_path(self, X)\n",
      "     |      Return the decision path in the forest\n",
      "     |      \n",
      "     |      .. versionadded:: 0.18\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      indicator : sparse csr array, shape = [n_samples, n_nodes]\n",
      "     |          Return a node indicator matrix where non zero elements\n",
      "     |          indicates that the samples goes through the nodes.\n",
      "     |      \n",
      "     |      n_nodes_ptr : array of size (n_estimators + 1, )\n",
      "     |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      "     |          gives the indicator value for the i-th estimator.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Build a forest of trees from the training set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csc_matrix``.\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples] or [n_samples, n_outputs]\n",
      "     |          The target values (class labels in classification, real numbers in\n",
      "     |          regression).\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples] or None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node. In the case of\n",
      "     |          classification, splits are also ignored if they would result in any\n",
      "     |          single class carrying a negative weight in either child node.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseForest:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Return the feature importances (the higher, the more important the\n",
      "     |         feature).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array, shape = [n_features]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble.base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Returns the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Returns iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Returns the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class GradientBoostingClassifier(BaseGradientBoosting, sklearn.base.ClassifierMixin)\n",
      "     |  Gradient Boosting for classification.\n",
      "     |  \n",
      "     |  GB builds an additive model in a\n",
      "     |  forward stage-wise fashion; it allows for the optimization of\n",
      "     |  arbitrary differentiable loss functions. In each stage ``n_classes_``\n",
      "     |  regression trees are fit on the negative gradient of the\n",
      "     |  binomial or multinomial deviance loss function. Binary classification\n",
      "     |  is a special case where only a single regression tree is induced.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <gradient_boosting>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  loss : {'deviance', 'exponential'}, optional (default='deviance')\n",
      "     |      loss function to be optimized. 'deviance' refers to\n",
      "     |      deviance (= logistic regression) for classification\n",
      "     |      with probabilistic outputs. For loss 'exponential' gradient\n",
      "     |      boosting recovers the AdaBoost algorithm.\n",
      "     |  \n",
      "     |  learning_rate : float, optional (default=0.1)\n",
      "     |      learning rate shrinks the contribution of each tree by `learning_rate`.\n",
      "     |      There is a trade-off between learning_rate and n_estimators.\n",
      "     |  \n",
      "     |  n_estimators : int (default=100)\n",
      "     |      The number of boosting stages to perform. Gradient boosting\n",
      "     |      is fairly robust to over-fitting so a large number usually\n",
      "     |      results in better performance.\n",
      "     |  \n",
      "     |  max_depth : integer, optional (default=3)\n",
      "     |      maximum depth of the individual regression estimators. The maximum\n",
      "     |      depth limits the number of nodes in the tree. Tune this parameter\n",
      "     |      for best performance; the best value depends on the interaction\n",
      "     |      of the input variables.\n",
      "     |  \n",
      "     |  criterion : string, optional (default=\"friedman_mse\")\n",
      "     |      The function to measure the quality of a split. Supported criteria\n",
      "     |      are \"friedman_mse\" for the mean squared error with improvement\n",
      "     |      score by Friedman, \"mse\" for mean squared error, and \"mae\" for\n",
      "     |      the mean absolute error. The default value of \"friedman_mse\" is\n",
      "     |      generally the best as it can provide a better approximation in\n",
      "     |      some cases.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |  \n",
      "     |  min_samples_split : int, float, optional (default=2)\n",
      "     |      The minimum number of samples required to split an internal node:\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_split` as the minimum number.\n",
      "     |      - If float, then `min_samples_split` is a percentage and\n",
      "     |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      "     |        number of samples for each split.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for percentages.\n",
      "     |  \n",
      "     |  min_samples_leaf : int, float, optional (default=1)\n",
      "     |      The minimum number of samples required to be at a leaf node:\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "     |      - If float, then `min_samples_leaf` is a percentage and\n",
      "     |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "     |        number of samples for each node.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for percentages.\n",
      "     |  \n",
      "     |  min_weight_fraction_leaf : float, optional (default=0.)\n",
      "     |      The minimum weighted fraction of the sum total of weights (of all\n",
      "     |      the input samples) required to be at a leaf node. Samples have\n",
      "     |      equal weight when sample_weight is not provided.\n",
      "     |  \n",
      "     |  subsample : float, optional (default=1.0)\n",
      "     |      The fraction of samples to be used for fitting the individual base\n",
      "     |      learners. If smaller than 1.0 this results in Stochastic Gradient\n",
      "     |      Boosting. `subsample` interacts with the parameter `n_estimators`.\n",
      "     |      Choosing `subsample < 1.0` leads to a reduction of variance\n",
      "     |      and an increase in bias.\n",
      "     |  \n",
      "     |  max_features : int, float, string or None, optional (default=None)\n",
      "     |      The number of features to consider when looking for the best split:\n",
      "     |  \n",
      "     |      - If int, then consider `max_features` features at each split.\n",
      "     |      - If float, then `max_features` is a percentage and\n",
      "     |        `int(max_features * n_features)` features are considered at each\n",
      "     |        split.\n",
      "     |      - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      "     |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "     |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      "     |      - If None, then `max_features=n_features`.\n",
      "     |  \n",
      "     |      Choosing `max_features < n_features` leads to a reduction of variance\n",
      "     |      and an increase in bias.\n",
      "     |  \n",
      "     |      Note: the search for a split does not stop until at least one\n",
      "     |      valid partition of the node samples is found, even if it requires to\n",
      "     |      effectively inspect more than ``max_features`` features.\n",
      "     |  \n",
      "     |  max_leaf_nodes : int or None, optional (default=None)\n",
      "     |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "     |      Best nodes are defined as relative reduction in impurity.\n",
      "     |      If None then unlimited number of leaf nodes.\n",
      "     |  \n",
      "     |  min_impurity_split : float,\n",
      "     |      Threshold for early stopping in tree growth. A node will split\n",
      "     |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      "     |  \n",
      "     |      .. deprecated:: 0.19\n",
      "     |         ``min_impurity_split`` has been deprecated in favor of\n",
      "     |         ``min_impurity_decrease`` in 0.19 and will be removed in 0.21.\n",
      "     |         Use ``min_impurity_decrease`` instead.\n",
      "     |  \n",
      "     |  min_impurity_decrease : float, optional (default=0.)\n",
      "     |      A node will be split if this split induces a decrease of the impurity\n",
      "     |      greater than or equal to this value.\n",
      "     |  \n",
      "     |      The weighted impurity decrease equation is the following::\n",
      "     |  \n",
      "     |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "     |                              - N_t_L / N_t * left_impurity)\n",
      "     |  \n",
      "     |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "     |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "     |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "     |  \n",
      "     |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "     |      if ``sample_weight`` is passed.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  init : BaseEstimator, None, optional (default=None)\n",
      "     |      An estimator object that is used to compute the initial\n",
      "     |      predictions. ``init`` has to provide ``fit`` and ``predict``.\n",
      "     |      If None it uses ``loss.init_estimator``.\n",
      "     |  \n",
      "     |  verbose : int, default: 0\n",
      "     |      Enable verbose output. If 1 then it prints progress and performance\n",
      "     |      once in a while (the more trees the lower the frequency). If greater\n",
      "     |      than 1 then it prints progress and performance for every tree.\n",
      "     |  \n",
      "     |  warm_start : bool, default: False\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit\n",
      "     |      and add more estimators to the ensemble, otherwise, just erase the\n",
      "     |      previous solution.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional (default=None)\n",
      "     |      If int, random_state is the seed used by the random number generator;\n",
      "     |      If RandomState instance, random_state is the random number generator;\n",
      "     |      If None, the random number generator is the RandomState instance used\n",
      "     |      by `np.random`.\n",
      "     |  \n",
      "     |  presort : bool or 'auto', optional (default='auto')\n",
      "     |      Whether to presort the data to speed up the finding of best splits in\n",
      "     |      fitting. Auto mode by default will use presorting on dense data and\n",
      "     |      default to normal sorting on sparse data. Setting presort to true on\n",
      "     |      sparse data will raise an error.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         *presort* parameter.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  feature_importances_ : array, shape = [n_features]\n",
      "     |      The feature importances (the higher, the more important the feature).\n",
      "     |  \n",
      "     |  oob_improvement_ : array, shape = [n_estimators]\n",
      "     |      The improvement in loss (= deviance) on the out-of-bag samples\n",
      "     |      relative to the previous iteration.\n",
      "     |      ``oob_improvement_[0]`` is the improvement in\n",
      "     |      loss of the first stage over the ``init`` estimator.\n",
      "     |  \n",
      "     |  train_score_ : array, shape = [n_estimators]\n",
      "     |      The i-th score ``train_score_[i]`` is the deviance (= loss) of the\n",
      "     |      model at iteration ``i`` on the in-bag sample.\n",
      "     |      If ``subsample == 1`` this is the deviance on the training data.\n",
      "     |  \n",
      "     |  loss_ : LossFunction\n",
      "     |      The concrete ``LossFunction`` object.\n",
      "     |  \n",
      "     |  init : BaseEstimator\n",
      "     |      The estimator that provides the initial predictions.\n",
      "     |      Set via the ``init`` argument or ``loss.init_estimator``.\n",
      "     |  \n",
      "     |  estimators_ : ndarray of DecisionTreeRegressor, shape = [n_estimators, ``loss_.K``]\n",
      "     |      The collection of fitted sub-estimators. ``loss_.K`` is 1 for binary\n",
      "     |      classification, otherwise n_classes.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The features are always randomly permuted at each split. Therefore,\n",
      "     |  the best found split may vary, even with the same training data and\n",
      "     |  ``max_features=n_features``, if the improvement of the criterion is\n",
      "     |  identical for several splits enumerated during the search of the best\n",
      "     |  split. To obtain a deterministic behaviour during fitting,\n",
      "     |  ``random_state`` has to be fixed.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  sklearn.tree.DecisionTreeClassifier, RandomForestClassifier\n",
      "     |  AdaBoostClassifier\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  J. Friedman, Greedy Function Approximation: A Gradient Boosting\n",
      "     |  Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n",
      "     |  \n",
      "     |  J. Friedman, Stochastic Gradient Boosting, 1999\n",
      "     |  \n",
      "     |  T. Hastie, R. Tibshirani and J. Friedman.\n",
      "     |  Elements of Statistical Learning Ed. 2, Springer, 2009.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GradientBoostingClassifier\n",
      "     |      BaseGradientBoosting\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.ensemble.base.BaseEnsemble\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, presort='auto')\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Compute the decision function of ``X``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : array, shape = [n_samples, n_classes] or [n_samples]\n",
      "     |          The decision function of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute `classes_`.\n",
      "     |          Regression and binary classification produce an array of shape\n",
      "     |          [n_samples].\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class for X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array of shape = [n_samples]\n",
      "     |          The predicted values.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Predict class log-probabilities for X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Raises\n",
      "     |      ------\n",
      "     |      AttributeError\n",
      "     |          If the ``loss`` does not support probabilities.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : array of shape = [n_samples]\n",
      "     |          The class log-probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute `classes_`.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Predict class probabilities for X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Raises\n",
      "     |      ------\n",
      "     |      AttributeError\n",
      "     |          If the ``loss`` does not support probabilities.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : array of shape = [n_samples]\n",
      "     |          The class probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute `classes_`.\n",
      "     |  \n",
      "     |  staged_decision_function(self, X)\n",
      "     |      Compute decision function of ``X`` for each iteration.\n",
      "     |      \n",
      "     |      This method allows monitoring (i.e. determine error on testing set)\n",
      "     |      after each stage.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : generator of array, shape = [n_samples, k]\n",
      "     |          The decision function of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute `classes_`.\n",
      "     |          Regression and binary classification are special cases with\n",
      "     |          ``k == 1``, otherwise ``k==n_classes``.\n",
      "     |  \n",
      "     |  staged_predict(self, X)\n",
      "     |      Predict class at each stage for X.\n",
      "     |      \n",
      "     |      This method allows monitoring (i.e. determine error on testing set)\n",
      "     |      after each stage.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : generator of array of shape = [n_samples]\n",
      "     |          The predicted value of the input samples.\n",
      "     |  \n",
      "     |  staged_predict_proba(self, X)\n",
      "     |      Predict class probabilities at each stage for X.\n",
      "     |      \n",
      "     |      This method allows monitoring (i.e. determine error on testing set)\n",
      "     |      after each stage.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : generator of array of shape = [n_samples]\n",
      "     |          The predicted value of the input samples.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseGradientBoosting:\n",
      "     |  \n",
      "     |  apply(self, X)\n",
      "     |      Apply trees in the ensemble to X, return leaf indices.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.17\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will\n",
      "     |          be converted to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape = [n_samples, n_estimators, n_classes]\n",
      "     |          For each datapoint x in X and for each tree in the ensemble,\n",
      "     |          return the index of the leaf x ends up in each estimator.\n",
      "     |          In the case of binary classification n_classes is 1.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, monitor=None)\n",
      "     |      Fit the gradient boosting model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = [n_samples, n_features]\n",
      "     |          Training vectors, where n_samples is the number of samples\n",
      "     |          and n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples]\n",
      "     |          Target values (integers in classification, real numbers in\n",
      "     |          regression)\n",
      "     |          For classification, labels must correspond to classes.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples] or None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node. In the case of\n",
      "     |          classification, splits are also ignored if they would result in any\n",
      "     |          single class carrying a negative weight in either child node.\n",
      "     |      \n",
      "     |      monitor : callable, optional\n",
      "     |          The monitor is called after each iteration with the current\n",
      "     |          iteration, a reference to the estimator and the local variables of\n",
      "     |          ``_fit_stages`` as keyword arguments ``callable(i, self,\n",
      "     |          locals())``. If the callable returns ``True`` the fitting procedure\n",
      "     |          is stopped. The monitor can be used for various things such as\n",
      "     |          computing held-out estimates, early stopping, model introspect, and\n",
      "     |          snapshoting.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseGradientBoosting:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Return the feature importances (the higher, the more important the\n",
      "     |         feature).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array, shape = [n_features]\n",
      "     |  \n",
      "     |  n_features\n",
      "     |      DEPRECATED: Attribute n_features was deprecated in version 0.19 and will be removed in 0.21.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble.base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Returns the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Returns iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Returns the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "    \n",
      "    class GradientBoostingRegressor(BaseGradientBoosting, sklearn.base.RegressorMixin)\n",
      "     |  Gradient Boosting for regression.\n",
      "     |  \n",
      "     |  GB builds an additive model in a forward stage-wise fashion;\n",
      "     |  it allows for the optimization of arbitrary differentiable loss functions.\n",
      "     |  In each stage a regression tree is fit on the negative gradient of the\n",
      "     |  given loss function.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <gradient_boosting>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  loss : {'ls', 'lad', 'huber', 'quantile'}, optional (default='ls')\n",
      "     |      loss function to be optimized. 'ls' refers to least squares\n",
      "     |      regression. 'lad' (least absolute deviation) is a highly robust\n",
      "     |      loss function solely based on order information of the input\n",
      "     |      variables. 'huber' is a combination of the two. 'quantile'\n",
      "     |      allows quantile regression (use `alpha` to specify the quantile).\n",
      "     |  \n",
      "     |  learning_rate : float, optional (default=0.1)\n",
      "     |      learning rate shrinks the contribution of each tree by `learning_rate`.\n",
      "     |      There is a trade-off between learning_rate and n_estimators.\n",
      "     |  \n",
      "     |  n_estimators : int (default=100)\n",
      "     |      The number of boosting stages to perform. Gradient boosting\n",
      "     |      is fairly robust to over-fitting so a large number usually\n",
      "     |      results in better performance.\n",
      "     |  \n",
      "     |  max_depth : integer, optional (default=3)\n",
      "     |      maximum depth of the individual regression estimators. The maximum\n",
      "     |      depth limits the number of nodes in the tree. Tune this parameter\n",
      "     |      for best performance; the best value depends on the interaction\n",
      "     |      of the input variables.\n",
      "     |  \n",
      "     |  criterion : string, optional (default=\"friedman_mse\")\n",
      "     |      The function to measure the quality of a split. Supported criteria\n",
      "     |      are \"friedman_mse\" for the mean squared error with improvement\n",
      "     |      score by Friedman, \"mse\" for mean squared error, and \"mae\" for\n",
      "     |      the mean absolute error. The default value of \"friedman_mse\" is\n",
      "     |      generally the best as it can provide a better approximation in\n",
      "     |      some cases.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |  \n",
      "     |  min_samples_split : int, float, optional (default=2)\n",
      "     |      The minimum number of samples required to split an internal node:\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_split` as the minimum number.\n",
      "     |      - If float, then `min_samples_split` is a percentage and\n",
      "     |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      "     |        number of samples for each split.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for percentages.\n",
      "     |  \n",
      "     |  min_samples_leaf : int, float, optional (default=1)\n",
      "     |      The minimum number of samples required to be at a leaf node:\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "     |      - If float, then `min_samples_leaf` is a percentage and\n",
      "     |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "     |        number of samples for each node.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for percentages.\n",
      "     |  \n",
      "     |  min_weight_fraction_leaf : float, optional (default=0.)\n",
      "     |      The minimum weighted fraction of the sum total of weights (of all\n",
      "     |      the input samples) required to be at a leaf node. Samples have\n",
      "     |      equal weight when sample_weight is not provided.\n",
      "     |  \n",
      "     |  subsample : float, optional (default=1.0)\n",
      "     |      The fraction of samples to be used for fitting the individual base\n",
      "     |      learners. If smaller than 1.0 this results in Stochastic Gradient\n",
      "     |      Boosting. `subsample` interacts with the parameter `n_estimators`.\n",
      "     |      Choosing `subsample < 1.0` leads to a reduction of variance\n",
      "     |      and an increase in bias.\n",
      "     |  \n",
      "     |  max_features : int, float, string or None, optional (default=None)\n",
      "     |      The number of features to consider when looking for the best split:\n",
      "     |  \n",
      "     |      - If int, then consider `max_features` features at each split.\n",
      "     |      - If float, then `max_features` is a percentage and\n",
      "     |        `int(max_features * n_features)` features are considered at each\n",
      "     |        split.\n",
      "     |      - If \"auto\", then `max_features=n_features`.\n",
      "     |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "     |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      "     |      - If None, then `max_features=n_features`.\n",
      "     |  \n",
      "     |      Choosing `max_features < n_features` leads to a reduction of variance\n",
      "     |      and an increase in bias.\n",
      "     |  \n",
      "     |      Note: the search for a split does not stop until at least one\n",
      "     |      valid partition of the node samples is found, even if it requires to\n",
      "     |      effectively inspect more than ``max_features`` features.\n",
      "     |  \n",
      "     |  max_leaf_nodes : int or None, optional (default=None)\n",
      "     |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "     |      Best nodes are defined as relative reduction in impurity.\n",
      "     |      If None then unlimited number of leaf nodes.\n",
      "     |  \n",
      "     |  min_impurity_split : float,\n",
      "     |      Threshold for early stopping in tree growth. A node will split\n",
      "     |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      "     |  \n",
      "     |      .. deprecated:: 0.19\n",
      "     |         ``min_impurity_split`` has been deprecated in favor of\n",
      "     |         ``min_impurity_decrease`` in 0.19 and will be removed in 0.21.\n",
      "     |         Use ``min_impurity_decrease`` instead.\n",
      "     |  \n",
      "     |  min_impurity_decrease : float, optional (default=0.)\n",
      "     |      A node will be split if this split induces a decrease of the impurity\n",
      "     |      greater than or equal to this value.\n",
      "     |  \n",
      "     |      The weighted impurity decrease equation is the following::\n",
      "     |  \n",
      "     |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "     |                              - N_t_L / N_t * left_impurity)\n",
      "     |  \n",
      "     |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "     |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "     |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "     |  \n",
      "     |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "     |      if ``sample_weight`` is passed.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  alpha : float (default=0.9)\n",
      "     |      The alpha-quantile of the huber loss function and the quantile\n",
      "     |      loss function. Only if ``loss='huber'`` or ``loss='quantile'``.\n",
      "     |  \n",
      "     |  init : BaseEstimator, None, optional (default=None)\n",
      "     |      An estimator object that is used to compute the initial\n",
      "     |      predictions. ``init`` has to provide ``fit`` and ``predict``.\n",
      "     |      If None it uses ``loss.init_estimator``.\n",
      "     |  \n",
      "     |  verbose : int, default: 0\n",
      "     |      Enable verbose output. If 1 then it prints progress and performance\n",
      "     |      once in a while (the more trees the lower the frequency). If greater\n",
      "     |      than 1 then it prints progress and performance for every tree.\n",
      "     |  \n",
      "     |  warm_start : bool, default: False\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit\n",
      "     |      and add more estimators to the ensemble, otherwise, just erase the\n",
      "     |      previous solution.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional (default=None)\n",
      "     |      If int, random_state is the seed used by the random number generator;\n",
      "     |      If RandomState instance, random_state is the random number generator;\n",
      "     |      If None, the random number generator is the RandomState instance used\n",
      "     |      by `np.random`.\n",
      "     |  \n",
      "     |  presort : bool or 'auto', optional (default='auto')\n",
      "     |      Whether to presort the data to speed up the finding of best splits in\n",
      "     |      fitting. Auto mode by default will use presorting on dense data and\n",
      "     |      default to normal sorting on sparse data. Setting presort to true on\n",
      "     |      sparse data will raise an error.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         optional parameter *presort*.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  feature_importances_ : array, shape = [n_features]\n",
      "     |      The feature importances (the higher, the more important the feature).\n",
      "     |  \n",
      "     |  oob_improvement_ : array, shape = [n_estimators]\n",
      "     |      The improvement in loss (= deviance) on the out-of-bag samples\n",
      "     |      relative to the previous iteration.\n",
      "     |      ``oob_improvement_[0]`` is the improvement in\n",
      "     |      loss of the first stage over the ``init`` estimator.\n",
      "     |  \n",
      "     |  train_score_ : array, shape = [n_estimators]\n",
      "     |      The i-th score ``train_score_[i]`` is the deviance (= loss) of the\n",
      "     |      model at iteration ``i`` on the in-bag sample.\n",
      "     |      If ``subsample == 1`` this is the deviance on the training data.\n",
      "     |  \n",
      "     |  loss_ : LossFunction\n",
      "     |      The concrete ``LossFunction`` object.\n",
      "     |  \n",
      "     |  init : BaseEstimator\n",
      "     |      The estimator that provides the initial predictions.\n",
      "     |      Set via the ``init`` argument or ``loss.init_estimator``.\n",
      "     |  \n",
      "     |  estimators_ : ndarray of DecisionTreeRegressor, shape = [n_estimators, 1]\n",
      "     |      The collection of fitted sub-estimators.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The features are always randomly permuted at each split. Therefore,\n",
      "     |  the best found split may vary, even with the same training data and\n",
      "     |  ``max_features=n_features``, if the improvement of the criterion is\n",
      "     |  identical for several splits enumerated during the search of the best\n",
      "     |  split. To obtain a deterministic behaviour during fitting,\n",
      "     |  ``random_state`` has to be fixed.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  DecisionTreeRegressor, RandomForestRegressor\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  J. Friedman, Greedy Function Approximation: A Gradient Boosting\n",
      "     |  Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n",
      "     |  \n",
      "     |  J. Friedman, Stochastic Gradient Boosting, 1999\n",
      "     |  \n",
      "     |  T. Hastie, R. Tibshirani and J. Friedman.\n",
      "     |  Elements of Statistical Learning Ed. 2, Springer, 2009.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GradientBoostingRegressor\n",
      "     |      BaseGradientBoosting\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.ensemble.base.BaseEnsemble\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, loss='ls', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, presort='auto')\n",
      "     |  \n",
      "     |  apply(self, X)\n",
      "     |      Apply trees in the ensemble to X, return leaf indices.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.17\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will\n",
      "     |          be converted to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape = [n_samples, n_estimators]\n",
      "     |          For each datapoint x in X and for each tree in the ensemble,\n",
      "     |          return the index of the leaf x ends up in each estimator.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict regression target for X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array of shape = [n_samples]\n",
      "     |          The predicted values.\n",
      "     |  \n",
      "     |  staged_predict(self, X)\n",
      "     |      Predict regression target at each stage for X.\n",
      "     |      \n",
      "     |      This method allows monitoring (i.e. determine error on testing set)\n",
      "     |      after each stage.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : generator of array of shape = [n_samples]\n",
      "     |          The predicted value of the input samples.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseGradientBoosting:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, monitor=None)\n",
      "     |      Fit the gradient boosting model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = [n_samples, n_features]\n",
      "     |          Training vectors, where n_samples is the number of samples\n",
      "     |          and n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples]\n",
      "     |          Target values (integers in classification, real numbers in\n",
      "     |          regression)\n",
      "     |          For classification, labels must correspond to classes.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples] or None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node. In the case of\n",
      "     |          classification, splits are also ignored if they would result in any\n",
      "     |          single class carrying a negative weight in either child node.\n",
      "     |      \n",
      "     |      monitor : callable, optional\n",
      "     |          The monitor is called after each iteration with the current\n",
      "     |          iteration, a reference to the estimator and the local variables of\n",
      "     |          ``_fit_stages`` as keyword arguments ``callable(i, self,\n",
      "     |          locals())``. If the callable returns ``True`` the fitting procedure\n",
      "     |          is stopped. The monitor can be used for various things such as\n",
      "     |          computing held-out estimates, early stopping, model introspect, and\n",
      "     |          snapshoting.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseGradientBoosting:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Return the feature importances (the higher, the more important the\n",
      "     |         feature).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array, shape = [n_features]\n",
      "     |  \n",
      "     |  n_features\n",
      "     |      DEPRECATED: Attribute n_features was deprecated in version 0.19 and will be removed in 0.21.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble.base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Returns the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Returns iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Returns the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class IsolationForest(sklearn.ensemble.bagging.BaseBagging)\n",
      "     |  Isolation Forest Algorithm\n",
      "     |  \n",
      "     |  Return the anomaly score of each sample using the IsolationForest algorithm\n",
      "     |  \n",
      "     |  The IsolationForest 'isolates' observations by randomly selecting a feature\n",
      "     |  and then randomly selecting a split value between the maximum and minimum\n",
      "     |  values of the selected feature.\n",
      "     |  \n",
      "     |  Since recursive partitioning can be represented by a tree structure, the\n",
      "     |  number of splittings required to isolate a sample is equivalent to the path\n",
      "     |  length from the root node to the terminating node.\n",
      "     |  \n",
      "     |  This path length, averaged over a forest of such random trees, is a\n",
      "     |  measure of normality and our decision function.\n",
      "     |  \n",
      "     |  Random partitioning produces noticeably shorter paths for anomalies.\n",
      "     |  Hence, when a forest of random trees collectively produce shorter path\n",
      "     |  lengths for particular samples, they are highly likely to be anomalies.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <isolation_forest>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.18\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_estimators : int, optional (default=100)\n",
      "     |      The number of base estimators in the ensemble.\n",
      "     |  \n",
      "     |  max_samples : int or float, optional (default=\"auto\")\n",
      "     |      The number of samples to draw from X to train each base estimator.\n",
      "     |          - If int, then draw `max_samples` samples.\n",
      "     |          - If float, then draw `max_samples * X.shape[0]` samples.\n",
      "     |          - If \"auto\", then `max_samples=min(256, n_samples)`.\n",
      "     |  \n",
      "     |      If max_samples is larger than the number of samples provided,\n",
      "     |      all samples will be used for all trees (no sampling).\n",
      "     |  \n",
      "     |  contamination : float in (0., 0.5), optional (default=0.1)\n",
      "     |      The amount of contamination of the data set, i.e. the proportion\n",
      "     |      of outliers in the data set. Used when fitting to define the threshold\n",
      "     |      on the decision function.\n",
      "     |  \n",
      "     |  max_features : int or float, optional (default=1.0)\n",
      "     |      The number of features to draw from X to train each base estimator.\n",
      "     |  \n",
      "     |          - If int, then draw `max_features` features.\n",
      "     |          - If float, then draw `max_features * X.shape[1]` features.\n",
      "     |  \n",
      "     |  bootstrap : boolean, optional (default=False)\n",
      "     |      If True, individual trees are fit on random subsets of the training\n",
      "     |      data sampled with replacement. If False, sampling without replacement\n",
      "     |      is performed.\n",
      "     |  \n",
      "     |  n_jobs : integer, optional (default=1)\n",
      "     |      The number of jobs to run in parallel for both `fit` and `predict`.\n",
      "     |      If -1, then the number of jobs is set to the number of cores.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional (default=None)\n",
      "     |      If int, random_state is the seed used by the random number generator;\n",
      "     |      If RandomState instance, random_state is the random number generator;\n",
      "     |      If None, the random number generator is the RandomState instance used\n",
      "     |      by `np.random`.\n",
      "     |  \n",
      "     |  verbose : int, optional (default=0)\n",
      "     |      Controls the verbosity of the tree building process.\n",
      "     |  \n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimators_ : list of DecisionTreeClassifier\n",
      "     |      The collection of fitted sub-estimators.\n",
      "     |  \n",
      "     |  estimators_samples_ : list of arrays\n",
      "     |      The subset of drawn samples (i.e., the in-bag samples) for each base\n",
      "     |      estimator.\n",
      "     |  \n",
      "     |  max_samples_ : integer\n",
      "     |      The actual number of samples\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation forest.\"\n",
      "     |         Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on.\n",
      "     |  .. [2] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation-based\n",
      "     |         anomaly detection.\" ACM Transactions on Knowledge Discovery from\n",
      "     |         Data (TKDD) 6.1 (2012): 3.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      IsolationForest\n",
      "     |      sklearn.ensemble.bagging.BaseBagging\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.ensemble.base.BaseEnsemble\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_estimators=100, max_samples='auto', contamination=0.1, max_features=1.0, bootstrap=False, n_jobs=1, random_state=None, verbose=0)\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Average anomaly score of X of the base classifiers.\n",
      "     |      \n",
      "     |      The anomaly score of an input sample is computed as\n",
      "     |      the mean anomaly score of the trees in the forest.\n",
      "     |      \n",
      "     |      The measure of normality of an observation given a tree is the depth\n",
      "     |      of the leaf containing this observation, which is equivalent to\n",
      "     |      the number of splittings required to isolate this point. In case of\n",
      "     |      several observations n_left in the leaf, the average path length of\n",
      "     |      a n_left samples isolation tree is added.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          The training input samples. Sparse matrices are accepted only if\n",
      "     |          they are supported by the base estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      scores : array of shape (n_samples,)\n",
      "     |          The anomaly score of the input samples.\n",
      "     |          The lower, the more abnormal.\n",
      "     |  \n",
      "     |  fit(self, X, y=None, sample_weight=None)\n",
      "     |      Fit estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          The input samples. Use ``dtype=np.float32`` for maximum\n",
      "     |          efficiency. Sparse matrices are also supported, use sparse\n",
      "     |          ``csc_matrix`` for maximum efficiency.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples] or None\n",
      "     |          Sample weights. If None, then samples are equally weighted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict if a particular sample is an outlier or not.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      is_inlier : array, shape (n_samples,)\n",
      "     |          For each observations, tells whether or not (+1 or -1) it should\n",
      "     |          be considered as an inlier according to the fitted model.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.ensemble.bagging.BaseBagging:\n",
      "     |  \n",
      "     |  estimators_samples_\n",
      "     |      The subset of drawn samples for each base estimator.\n",
      "     |      \n",
      "     |      Returns a dynamically generated list of boolean masks identifying\n",
      "     |      the samples used for fitting each member of the ensemble, i.e.,\n",
      "     |      the in-bag samples.\n",
      "     |      \n",
      "     |      Note: the list is re-created at each call to the property in order\n",
      "     |      to reduce the object memory footprint by not storing the sampling\n",
      "     |      data. Thus fetching the property may be slower than expected.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble.base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Returns the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Returns iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Returns the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class RandomForestClassifier(ForestClassifier)\n",
      "     |  A random forest classifier.\n",
      "     |  \n",
      "     |  A random forest is a meta estimator that fits a number of decision tree\n",
      "     |  classifiers on various sub-samples of the dataset and use averaging to\n",
      "     |  improve the predictive accuracy and control over-fitting.\n",
      "     |  The sub-sample size is always the same as the original\n",
      "     |  input sample size but the samples are drawn with replacement if\n",
      "     |  `bootstrap=True` (default).\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <forest>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_estimators : integer, optional (default=10)\n",
      "     |      The number of trees in the forest.\n",
      "     |  \n",
      "     |  criterion : string, optional (default=\"gini\")\n",
      "     |      The function to measure the quality of a split. Supported criteria are\n",
      "     |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
      "     |      Note: this parameter is tree-specific.\n",
      "     |  \n",
      "     |  max_features : int, float, string or None, optional (default=\"auto\")\n",
      "     |      The number of features to consider when looking for the best split:\n",
      "     |  \n",
      "     |      - If int, then consider `max_features` features at each split.\n",
      "     |      - If float, then `max_features` is a percentage and\n",
      "     |        `int(max_features * n_features)` features are considered at each\n",
      "     |        split.\n",
      "     |      - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      "     |      - If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").\n",
      "     |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      "     |      - If None, then `max_features=n_features`.\n",
      "     |  \n",
      "     |      Note: the search for a split does not stop until at least one\n",
      "     |      valid partition of the node samples is found, even if it requires to\n",
      "     |      effectively inspect more than ``max_features`` features.\n",
      "     |  \n",
      "     |  max_depth : integer or None, optional (default=None)\n",
      "     |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      "     |      all leaves are pure or until all leaves contain less than\n",
      "     |      min_samples_split samples.\n",
      "     |  \n",
      "     |  min_samples_split : int, float, optional (default=2)\n",
      "     |      The minimum number of samples required to split an internal node:\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_split` as the minimum number.\n",
      "     |      - If float, then `min_samples_split` is a percentage and\n",
      "     |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      "     |        number of samples for each split.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for percentages.\n",
      "     |  \n",
      "     |  min_samples_leaf : int, float, optional (default=1)\n",
      "     |      The minimum number of samples required to be at a leaf node:\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "     |      - If float, then `min_samples_leaf` is a percentage and\n",
      "     |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "     |        number of samples for each node.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for percentages.\n",
      "     |  \n",
      "     |  min_weight_fraction_leaf : float, optional (default=0.)\n",
      "     |      The minimum weighted fraction of the sum total of weights (of all\n",
      "     |      the input samples) required to be at a leaf node. Samples have\n",
      "     |      equal weight when sample_weight is not provided.\n",
      "     |  \n",
      "     |  max_leaf_nodes : int or None, optional (default=None)\n",
      "     |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "     |      Best nodes are defined as relative reduction in impurity.\n",
      "     |      If None then unlimited number of leaf nodes.\n",
      "     |  \n",
      "     |  min_impurity_split : float,\n",
      "     |      Threshold for early stopping in tree growth. A node will split\n",
      "     |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      "     |  \n",
      "     |      .. deprecated:: 0.19\n",
      "     |         ``min_impurity_split`` has been deprecated in favor of\n",
      "     |         ``min_impurity_decrease`` in 0.19 and will be removed in 0.21.\n",
      "     |         Use ``min_impurity_decrease`` instead.\n",
      "     |  \n",
      "     |  min_impurity_decrease : float, optional (default=0.)\n",
      "     |      A node will be split if this split induces a decrease of the impurity\n",
      "     |      greater than or equal to this value.\n",
      "     |  \n",
      "     |      The weighted impurity decrease equation is the following::\n",
      "     |  \n",
      "     |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "     |                              - N_t_L / N_t * left_impurity)\n",
      "     |  \n",
      "     |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "     |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "     |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "     |  \n",
      "     |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "     |      if ``sample_weight`` is passed.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  bootstrap : boolean, optional (default=True)\n",
      "     |      Whether bootstrap samples are used when building trees.\n",
      "     |  \n",
      "     |  oob_score : bool (default=False)\n",
      "     |      Whether to use out-of-bag samples to estimate\n",
      "     |      the generalization accuracy.\n",
      "     |  \n",
      "     |  n_jobs : integer, optional (default=1)\n",
      "     |      The number of jobs to run in parallel for both `fit` and `predict`.\n",
      "     |      If -1, then the number of jobs is set to the number of cores.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional (default=None)\n",
      "     |      If int, random_state is the seed used by the random number generator;\n",
      "     |      If RandomState instance, random_state is the random number generator;\n",
      "     |      If None, the random number generator is the RandomState instance used\n",
      "     |      by `np.random`.\n",
      "     |  \n",
      "     |  verbose : int, optional (default=0)\n",
      "     |      Controls the verbosity of the tree building process.\n",
      "     |  \n",
      "     |  warm_start : bool, optional (default=False)\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit\n",
      "     |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      "     |      new forest.\n",
      "     |  \n",
      "     |  class_weight : dict, list of dicts, \"balanced\",\n",
      "     |      \"balanced_subsample\" or None, optional (default=None)\n",
      "     |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      "     |      If not given, all classes are supposed to have weight one. For\n",
      "     |      multi-output problems, a list of dicts can be provided in the same\n",
      "     |      order as the columns of y.\n",
      "     |  \n",
      "     |      Note that for multioutput (including multilabel) weights should be\n",
      "     |      defined for each class of every column in its own dict. For example,\n",
      "     |      for four-class multilabel classification weights should be\n",
      "     |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
      "     |      [{1:1}, {2:5}, {3:1}, {4:1}].\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      "     |  \n",
      "     |      The \"balanced_subsample\" mode is the same as \"balanced\" except that\n",
      "     |      weights are computed based on the bootstrap sample for every tree\n",
      "     |      grown.\n",
      "     |  \n",
      "     |      For multi-output, the weights of each column of y will be multiplied.\n",
      "     |  \n",
      "     |      Note that these weights will be multiplied with sample_weight (passed\n",
      "     |      through the fit method) if sample_weight is specified.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimators_ : list of DecisionTreeClassifier\n",
      "     |      The collection of fitted sub-estimators.\n",
      "     |  \n",
      "     |  classes_ : array of shape = [n_classes] or a list of such arrays\n",
      "     |      The classes labels (single output problem), or a list of arrays of\n",
      "     |      class labels (multi-output problem).\n",
      "     |  \n",
      "     |  n_classes_ : int or list\n",
      "     |      The number of classes (single output problem), or a list containing the\n",
      "     |      number of classes for each output (multi-output problem).\n",
      "     |  \n",
      "     |  n_features_ : int\n",
      "     |      The number of features when ``fit`` is performed.\n",
      "     |  \n",
      "     |  n_outputs_ : int\n",
      "     |      The number of outputs when ``fit`` is performed.\n",
      "     |  \n",
      "     |  feature_importances_ : array of shape = [n_features]\n",
      "     |      The feature importances (the higher, the more important the feature).\n",
      "     |  \n",
      "     |  oob_score_ : float\n",
      "     |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      "     |  \n",
      "     |  oob_decision_function_ : array of shape = [n_samples, n_classes]\n",
      "     |      Decision function computed with out-of-bag estimate on the training\n",
      "     |      set. If n_estimators is small it might be possible that a data point\n",
      "     |      was never left out during the bootstrap. In this case,\n",
      "     |      `oob_decision_function_` might contain NaN.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.ensemble import RandomForestClassifier\n",
      "     |  >>> from sklearn.datasets import make_classification\n",
      "     |  >>>\n",
      "     |  >>> X, y = make_classification(n_samples=1000, n_features=4,\n",
      "     |  ...                            n_informative=2, n_redundant=0,\n",
      "     |  ...                            random_state=0, shuffle=False)\n",
      "     |  >>> clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "     |              max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
      "     |              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "     |              min_samples_leaf=1, min_samples_split=2,\n",
      "     |              min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "     |              oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
      "     |  >>> print(clf.feature_importances_)\n",
      "     |  [ 0.17287856  0.80608704  0.01884792  0.00218648]\n",
      "     |  >>> print(clf.predict([[0, 0, 0, 0]]))\n",
      "     |  [1]\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The default values for the parameters controlling the size of the trees\n",
      "     |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      "     |  unpruned trees which can potentially be very large on some data sets. To\n",
      "     |  reduce memory consumption, the complexity and size of the trees should be\n",
      "     |  controlled by setting those parameter values.\n",
      "     |  \n",
      "     |  The features are always randomly permuted at each split. Therefore,\n",
      "     |  the best found split may vary, even with the same training data,\n",
      "     |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      "     |  of the criterion is identical for several splits enumerated during the\n",
      "     |  search of the best split. To obtain a deterministic behaviour during\n",
      "     |  fitting, ``random_state`` has to be fixed.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  DecisionTreeClassifier, ExtraTreesClassifier\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RandomForestClassifier\n",
      "     |      ForestClassifier\n",
      "     |      abc.NewBase\n",
      "     |      BaseForest\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.ensemble.base.BaseEnsemble\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_estimators=10, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False, class_weight=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from ForestClassifier:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class for X.\n",
      "     |      \n",
      "     |      The predicted class of an input sample is a vote by the trees in\n",
      "     |      the forest, weighted by their probability estimates. That is,\n",
      "     |      the predicted class is the one with highest mean probability\n",
      "     |      estimate across the trees.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array of shape = [n_samples] or [n_samples, n_outputs]\n",
      "     |          The predicted classes.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Predict class log-probabilities for X.\n",
      "     |      \n",
      "     |      The predicted class log-probabilities of an input sample is computed as\n",
      "     |      the log of the mean predicted class probabilities of the trees in the\n",
      "     |      forest.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : array of shape = [n_samples, n_classes], or a list of n_outputs\n",
      "     |          such arrays if n_outputs > 1.\n",
      "     |          The class probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute `classes_`.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Predict class probabilities for X.\n",
      "     |      \n",
      "     |      The predicted class probabilities of an input sample are computed as\n",
      "     |      the mean predicted class probabilities of the trees in the forest. The\n",
      "     |      class probability of a single tree is the fraction of samples of the same\n",
      "     |      class in a leaf.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : array of shape = [n_samples, n_classes], or a list of n_outputs\n",
      "     |          such arrays if n_outputs > 1.\n",
      "     |          The class probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute `classes_`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseForest:\n",
      "     |  \n",
      "     |  apply(self, X)\n",
      "     |      Apply trees in the forest to X, return leaf indices.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape = [n_samples, n_estimators]\n",
      "     |          For each datapoint x in X and for each tree in the forest,\n",
      "     |          return the index of the leaf x ends up in.\n",
      "     |  \n",
      "     |  decision_path(self, X)\n",
      "     |      Return the decision path in the forest\n",
      "     |      \n",
      "     |      .. versionadded:: 0.18\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      indicator : sparse csr array, shape = [n_samples, n_nodes]\n",
      "     |          Return a node indicator matrix where non zero elements\n",
      "     |          indicates that the samples goes through the nodes.\n",
      "     |      \n",
      "     |      n_nodes_ptr : array of size (n_estimators + 1, )\n",
      "     |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      "     |          gives the indicator value for the i-th estimator.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Build a forest of trees from the training set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csc_matrix``.\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples] or [n_samples, n_outputs]\n",
      "     |          The target values (class labels in classification, real numbers in\n",
      "     |          regression).\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples] or None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node. In the case of\n",
      "     |          classification, splits are also ignored if they would result in any\n",
      "     |          single class carrying a negative weight in either child node.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseForest:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Return the feature importances (the higher, the more important the\n",
      "     |         feature).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array, shape = [n_features]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble.base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Returns the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Returns iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Returns the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "    \n",
      "    class RandomForestRegressor(ForestRegressor)\n",
      "     |  A random forest regressor.\n",
      "     |  \n",
      "     |  A random forest is a meta estimator that fits a number of classifying\n",
      "     |  decision trees on various sub-samples of the dataset and use averaging\n",
      "     |  to improve the predictive accuracy and control over-fitting.\n",
      "     |  The sub-sample size is always the same as the original\n",
      "     |  input sample size but the samples are drawn with replacement if\n",
      "     |  `bootstrap=True` (default).\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <forest>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_estimators : integer, optional (default=10)\n",
      "     |      The number of trees in the forest.\n",
      "     |  \n",
      "     |  criterion : string, optional (default=\"mse\")\n",
      "     |      The function to measure the quality of a split. Supported criteria\n",
      "     |      are \"mse\" for the mean squared error, which is equal to variance\n",
      "     |      reduction as feature selection criterion, and \"mae\" for the mean\n",
      "     |      absolute error.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |         Mean Absolute Error (MAE) criterion.\n",
      "     |  \n",
      "     |  max_features : int, float, string or None, optional (default=\"auto\")\n",
      "     |      The number of features to consider when looking for the best split:\n",
      "     |  \n",
      "     |      - If int, then consider `max_features` features at each split.\n",
      "     |      - If float, then `max_features` is a percentage and\n",
      "     |        `int(max_features * n_features)` features are considered at each\n",
      "     |        split.\n",
      "     |      - If \"auto\", then `max_features=n_features`.\n",
      "     |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "     |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      "     |      - If None, then `max_features=n_features`.\n",
      "     |  \n",
      "     |      Note: the search for a split does not stop until at least one\n",
      "     |      valid partition of the node samples is found, even if it requires to\n",
      "     |      effectively inspect more than ``max_features`` features.\n",
      "     |  \n",
      "     |  max_depth : integer or None, optional (default=None)\n",
      "     |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      "     |      all leaves are pure or until all leaves contain less than\n",
      "     |      min_samples_split samples.\n",
      "     |  \n",
      "     |  min_samples_split : int, float, optional (default=2)\n",
      "     |      The minimum number of samples required to split an internal node:\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_split` as the minimum number.\n",
      "     |      - If float, then `min_samples_split` is a percentage and\n",
      "     |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      "     |        number of samples for each split.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for percentages.\n",
      "     |  \n",
      "     |  min_samples_leaf : int, float, optional (default=1)\n",
      "     |      The minimum number of samples required to be at a leaf node:\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "     |      - If float, then `min_samples_leaf` is a percentage and\n",
      "     |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "     |        number of samples for each node.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for percentages.\n",
      "     |  \n",
      "     |  min_weight_fraction_leaf : float, optional (default=0.)\n",
      "     |      The minimum weighted fraction of the sum total of weights (of all\n",
      "     |      the input samples) required to be at a leaf node. Samples have\n",
      "     |      equal weight when sample_weight is not provided.\n",
      "     |  \n",
      "     |  max_leaf_nodes : int or None, optional (default=None)\n",
      "     |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "     |      Best nodes are defined as relative reduction in impurity.\n",
      "     |      If None then unlimited number of leaf nodes.\n",
      "     |  \n",
      "     |  min_impurity_split : float,\n",
      "     |      Threshold for early stopping in tree growth. A node will split\n",
      "     |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      "     |  \n",
      "     |      .. deprecated:: 0.19\n",
      "     |         ``min_impurity_split`` has been deprecated in favor of\n",
      "     |         ``min_impurity_decrease`` in 0.19 and will be removed in 0.21.\n",
      "     |         Use ``min_impurity_decrease`` instead.\n",
      "     |  \n",
      "     |  min_impurity_decrease : float, optional (default=0.)\n",
      "     |      A node will be split if this split induces a decrease of the impurity\n",
      "     |      greater than or equal to this value.\n",
      "     |  \n",
      "     |      The weighted impurity decrease equation is the following::\n",
      "     |  \n",
      "     |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "     |                              - N_t_L / N_t * left_impurity)\n",
      "     |  \n",
      "     |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "     |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "     |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "     |  \n",
      "     |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "     |      if ``sample_weight`` is passed.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  bootstrap : boolean, optional (default=True)\n",
      "     |      Whether bootstrap samples are used when building trees.\n",
      "     |  \n",
      "     |  oob_score : bool, optional (default=False)\n",
      "     |      whether to use out-of-bag samples to estimate\n",
      "     |      the R^2 on unseen data.\n",
      "     |  \n",
      "     |  n_jobs : integer, optional (default=1)\n",
      "     |      The number of jobs to run in parallel for both `fit` and `predict`.\n",
      "     |      If -1, then the number of jobs is set to the number of cores.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional (default=None)\n",
      "     |      If int, random_state is the seed used by the random number generator;\n",
      "     |      If RandomState instance, random_state is the random number generator;\n",
      "     |      If None, the random number generator is the RandomState instance used\n",
      "     |      by `np.random`.\n",
      "     |  \n",
      "     |  verbose : int, optional (default=0)\n",
      "     |      Controls the verbosity of the tree building process.\n",
      "     |  \n",
      "     |  warm_start : bool, optional (default=False)\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit\n",
      "     |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      "     |      new forest.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimators_ : list of DecisionTreeRegressor\n",
      "     |      The collection of fitted sub-estimators.\n",
      "     |  \n",
      "     |  feature_importances_ : array of shape = [n_features]\n",
      "     |      The feature importances (the higher, the more important the feature).\n",
      "     |  \n",
      "     |  n_features_ : int\n",
      "     |      The number of features when ``fit`` is performed.\n",
      "     |  \n",
      "     |  n_outputs_ : int\n",
      "     |      The number of outputs when ``fit`` is performed.\n",
      "     |  \n",
      "     |  oob_score_ : float\n",
      "     |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      "     |  \n",
      "     |  oob_prediction_ : array of shape = [n_samples]\n",
      "     |      Prediction computed with out-of-bag estimate on the training set.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.ensemble import RandomForestRegressor\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>>\n",
      "     |  >>> X, y = make_regression(n_features=4, n_informative=2,\n",
      "     |  ...                        random_state=0, shuffle=False)\n",
      "     |  >>> regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
      "     |  >>> regr.fit(X, y)\n",
      "     |  RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=2,\n",
      "     |             max_features='auto', max_leaf_nodes=None,\n",
      "     |             min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "     |             min_samples_leaf=1, min_samples_split=2,\n",
      "     |             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "     |             oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
      "     |  >>> print(regr.feature_importances_)\n",
      "     |  [ 0.17339552  0.81594114  0.          0.01066333]\n",
      "     |  >>> print(regr.predict([[0, 0, 0, 0]]))\n",
      "     |  [-2.50699856]\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The default values for the parameters controlling the size of the trees\n",
      "     |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      "     |  unpruned trees which can potentially be very large on some data sets. To\n",
      "     |  reduce memory consumption, the complexity and size of the trees should be\n",
      "     |  controlled by setting those parameter values.\n",
      "     |  \n",
      "     |  The features are always randomly permuted at each split. Therefore,\n",
      "     |  the best found split may vary, even with the same training data,\n",
      "     |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      "     |  of the criterion is identical for several splits enumerated during the\n",
      "     |  search of the best split. To obtain a deterministic behaviour during\n",
      "     |  fitting, ``random_state`` has to be fixed.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  DecisionTreeRegressor, ExtraTreesRegressor\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RandomForestRegressor\n",
      "     |      ForestRegressor\n",
      "     |      abc.NewBase\n",
      "     |      BaseForest\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.ensemble.base.BaseEnsemble\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_estimators=10, criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from ForestRegressor:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict regression target for X.\n",
      "     |      \n",
      "     |      The predicted regression target of an input sample is computed as the\n",
      "     |      mean predicted regression targets of the trees in the forest.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array of shape = [n_samples] or [n_samples, n_outputs]\n",
      "     |          The predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseForest:\n",
      "     |  \n",
      "     |  apply(self, X)\n",
      "     |      Apply trees in the forest to X, return leaf indices.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape = [n_samples, n_estimators]\n",
      "     |          For each datapoint x in X and for each tree in the forest,\n",
      "     |          return the index of the leaf x ends up in.\n",
      "     |  \n",
      "     |  decision_path(self, X)\n",
      "     |      Return the decision path in the forest\n",
      "     |      \n",
      "     |      .. versionadded:: 0.18\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      indicator : sparse csr array, shape = [n_samples, n_nodes]\n",
      "     |          Return a node indicator matrix where non zero elements\n",
      "     |          indicates that the samples goes through the nodes.\n",
      "     |      \n",
      "     |      n_nodes_ptr : array of size (n_estimators + 1, )\n",
      "     |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      "     |          gives the indicator value for the i-th estimator.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Build a forest of trees from the training set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "     |          The training input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csc_matrix``.\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples] or [n_samples, n_outputs]\n",
      "     |          The target values (class labels in classification, real numbers in\n",
      "     |          regression).\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples] or None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node. In the case of\n",
      "     |          classification, splits are also ignored if they would result in any\n",
      "     |          single class carrying a negative weight in either child node.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseForest:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Return the feature importances (the higher, the more important the\n",
      "     |         feature).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array, shape = [n_features]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble.base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Returns the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Returns iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Returns the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class RandomTreesEmbedding(BaseForest)\n",
      "     |  An ensemble of totally random trees.\n",
      "     |  \n",
      "     |  An unsupervised transformation of a dataset to a high-dimensional\n",
      "     |  sparse representation. A datapoint is coded according to which leaf of\n",
      "     |  each tree it is sorted into. Using a one-hot encoding of the leaves,\n",
      "     |  this leads to a binary coding with as many ones as there are trees in\n",
      "     |  the forest.\n",
      "     |  \n",
      "     |  The dimensionality of the resulting representation is\n",
      "     |  ``n_out <= n_estimators * max_leaf_nodes``. If ``max_leaf_nodes == None``,\n",
      "     |  the number of leaf nodes is at most ``n_estimators * 2 ** max_depth``.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <random_trees_embedding>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_estimators : integer, optional (default=10)\n",
      "     |      Number of trees in the forest.\n",
      "     |  \n",
      "     |  max_depth : integer, optional (default=5)\n",
      "     |      The maximum depth of each tree. If None, then nodes are expanded until\n",
      "     |      all leaves are pure or until all leaves contain less than\n",
      "     |      min_samples_split samples.\n",
      "     |  \n",
      "     |  min_samples_split : int, float, optional (default=2)\n",
      "     |      The minimum number of samples required to split an internal node:\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_split` as the minimum number.\n",
      "     |      - If float, then `min_samples_split` is a percentage and\n",
      "     |        `ceil(min_samples_split * n_samples)` is the minimum\n",
      "     |        number of samples for each split.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for percentages.\n",
      "     |  \n",
      "     |  min_samples_leaf : int, float, optional (default=1)\n",
      "     |      The minimum number of samples required to be at a leaf node:\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "     |      - If float, then `min_samples_leaf` is a percentage and\n",
      "     |        `ceil(min_samples_leaf * n_samples)` is the minimum\n",
      "     |        number of samples for each node.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for percentages.\n",
      "     |  \n",
      "     |  min_weight_fraction_leaf : float, optional (default=0.)\n",
      "     |      The minimum weighted fraction of the sum total of weights (of all\n",
      "     |      the input samples) required to be at a leaf node. Samples have\n",
      "     |      equal weight when sample_weight is not provided.\n",
      "     |  \n",
      "     |  max_leaf_nodes : int or None, optional (default=None)\n",
      "     |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "     |      Best nodes are defined as relative reduction in impurity.\n",
      "     |      If None then unlimited number of leaf nodes.\n",
      "     |  \n",
      "     |  min_impurity_split : float,\n",
      "     |      Threshold for early stopping in tree growth. A node will split\n",
      "     |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      "     |  \n",
      "     |      .. deprecated:: 0.19\n",
      "     |         ``min_impurity_split`` has been deprecated in favor of\n",
      "     |         ``min_impurity_decrease`` in 0.19 and will be removed in 0.21.\n",
      "     |         Use ``min_impurity_decrease`` instead.\n",
      "     |  \n",
      "     |  min_impurity_decrease : float, optional (default=0.)\n",
      "     |      A node will be split if this split induces a decrease of the impurity\n",
      "     |      greater than or equal to this value.\n",
      "     |  \n",
      "     |      The weighted impurity decrease equation is the following::\n",
      "     |  \n",
      "     |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "     |                              - N_t_L / N_t * left_impurity)\n",
      "     |  \n",
      "     |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "     |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "     |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "     |  \n",
      "     |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "     |      if ``sample_weight`` is passed.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  bootstrap : boolean, optional (default=True)\n",
      "     |      Whether bootstrap samples are used when building trees.\n",
      "     |  \n",
      "     |  sparse_output : bool, optional (default=True)\n",
      "     |      Whether or not to return a sparse CSR matrix, as default behavior,\n",
      "     |      or to return a dense array compatible with dense pipeline operators.\n",
      "     |  \n",
      "     |  n_jobs : integer, optional (default=1)\n",
      "     |      The number of jobs to run in parallel for both `fit` and `predict`.\n",
      "     |      If -1, then the number of jobs is set to the number of cores.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional (default=None)\n",
      "     |      If int, random_state is the seed used by the random number generator;\n",
      "     |      If RandomState instance, random_state is the random number generator;\n",
      "     |      If None, the random number generator is the RandomState instance used\n",
      "     |      by `np.random`.\n",
      "     |  \n",
      "     |  verbose : int, optional (default=0)\n",
      "     |      Controls the verbosity of the tree building process.\n",
      "     |  \n",
      "     |  warm_start : bool, optional (default=False)\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit\n",
      "     |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      "     |      new forest.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimators_ : list of DecisionTreeClassifier\n",
      "     |      The collection of fitted sub-estimators.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n",
      "     |         Machine Learning, 63(1), 3-42, 2006.\n",
      "     |  .. [2] Moosmann, F. and Triggs, B. and Jurie, F.  \"Fast discriminative\n",
      "     |         visual codebooks using randomized clustering forests\"\n",
      "     |         NIPS 2007\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RandomTreesEmbedding\n",
      "     |      BaseForest\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.ensemble.base.BaseEnsemble\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_estimators=10, max_depth=5, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, sparse_output=True, n_jobs=1, random_state=None, verbose=0, warm_start=False)\n",
      "     |  \n",
      "     |  fit(self, X, y=None, sample_weight=None)\n",
      "     |      Fit estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape=(n_samples, n_features)\n",
      "     |          The input samples. Use ``dtype=np.float32`` for maximum\n",
      "     |          efficiency. Sparse matrices are also supported, use sparse\n",
      "     |          ``csc_matrix`` for maximum efficiency.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples] or None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node. In the case of\n",
      "     |          classification, splits are also ignored if they would result in any\n",
      "     |          single class carrying a negative weight in either child node.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, sample_weight=None)\n",
      "     |      Fit estimator and transform dataset.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape=(n_samples, n_features)\n",
      "     |          Input data used to build forests. Use ``dtype=np.float32`` for\n",
      "     |          maximum efficiency.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples] or None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node. In the case of\n",
      "     |          classification, splits are also ignored if they would result in any\n",
      "     |          single class carrying a negative weight in either child node.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_transformed : sparse matrix, shape=(n_samples, n_out)\n",
      "     |          Transformed dataset.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Transform dataset.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape=(n_samples, n_features)\n",
      "     |          Input data to be transformed. Use ``dtype=np.float32`` for maximum\n",
      "     |          efficiency. Sparse matrices are also supported, use sparse\n",
      "     |          ``csr_matrix`` for maximum efficiency.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_transformed : sparse matrix, shape=(n_samples, n_out)\n",
      "     |          Transformed dataset.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseForest:\n",
      "     |  \n",
      "     |  apply(self, X)\n",
      "     |      Apply trees in the forest to X, return leaf indices.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape = [n_samples, n_estimators]\n",
      "     |          For each datapoint x in X and for each tree in the forest,\n",
      "     |          return the index of the leaf x ends up in.\n",
      "     |  \n",
      "     |  decision_path(self, X)\n",
      "     |      Return the decision path in the forest\n",
      "     |      \n",
      "     |      .. versionadded:: 0.18\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      indicator : sparse csr array, shape = [n_samples, n_nodes]\n",
      "     |          Return a node indicator matrix where non zero elements\n",
      "     |          indicates that the samples goes through the nodes.\n",
      "     |      \n",
      "     |      n_nodes_ptr : array of size (n_estimators + 1, )\n",
      "     |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      "     |          gives the indicator value for the i-th estimator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseForest:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Return the feature importances (the higher, the more important the\n",
      "     |         feature).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array, shape = [n_features]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble.base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Returns the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Returns iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Returns the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class VotingClassifier(sklearn.utils.metaestimators._BaseComposition, sklearn.base.ClassifierMixin, sklearn.base.TransformerMixin)\n",
      "     |  Soft Voting/Majority Rule classifier for unfitted estimators.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.17\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <voting_classifier>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  estimators : list of (string, estimator) tuples\n",
      "     |      Invoking the ``fit`` method on the ``VotingClassifier`` will fit clones\n",
      "     |      of those original estimators that will be stored in the class attribute\n",
      "     |      ``self.estimators_``. An estimator can be set to `None` using\n",
      "     |      ``set_params``.\n",
      "     |  \n",
      "     |  voting : str, {'hard', 'soft'} (default='hard')\n",
      "     |      If 'hard', uses predicted class labels for majority rule voting.\n",
      "     |      Else if 'soft', predicts the class label based on the argmax of\n",
      "     |      the sums of the predicted probabilities, which is recommended for\n",
      "     |      an ensemble of well-calibrated classifiers.\n",
      "     |  \n",
      "     |  weights : array-like, shape = [n_classifiers], optional (default=`None`)\n",
      "     |      Sequence of weights (`float` or `int`) to weight the occurrences of\n",
      "     |      predicted class labels (`hard` voting) or class probabilities\n",
      "     |      before averaging (`soft` voting). Uses uniform weights if `None`.\n",
      "     |  \n",
      "     |  n_jobs : int, optional (default=1)\n",
      "     |      The number of jobs to run in parallel for ``fit``.\n",
      "     |      If -1, then the number of jobs is set to the number of cores.\n",
      "     |  \n",
      "     |  flatten_transform : bool, optional (default=None)\n",
      "     |      Affects shape of transform output only when voting='soft'\n",
      "     |      If voting='soft' and flatten_transform=True, transform method returns\n",
      "     |      matrix with shape (n_samples, n_classifiers * n_classes). If\n",
      "     |      flatten_transform=False, it returns\n",
      "     |      (n_classifiers, n_samples, n_classes).\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimators_ : list of classifiers\n",
      "     |      The collection of fitted sub-estimators as defined in ``estimators``\n",
      "     |      that are not `None`.\n",
      "     |  \n",
      "     |  classes_ : array-like, shape = [n_predictions]\n",
      "     |      The classes labels.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.linear_model import LogisticRegression\n",
      "     |  >>> from sklearn.naive_bayes import GaussianNB\n",
      "     |  >>> from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
      "     |  >>> clf1 = LogisticRegression(random_state=1)\n",
      "     |  >>> clf2 = RandomForestClassifier(random_state=1)\n",
      "     |  >>> clf3 = GaussianNB()\n",
      "     |  >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
      "     |  >>> y = np.array([1, 1, 1, 2, 2, 2])\n",
      "     |  >>> eclf1 = VotingClassifier(estimators=[\n",
      "     |  ...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\n",
      "     |  >>> eclf1 = eclf1.fit(X, y)\n",
      "     |  >>> print(eclf1.predict(X))\n",
      "     |  [1 1 1 2 2 2]\n",
      "     |  >>> eclf2 = VotingClassifier(estimators=[\n",
      "     |  ...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n",
      "     |  ...         voting='soft')\n",
      "     |  >>> eclf2 = eclf2.fit(X, y)\n",
      "     |  >>> print(eclf2.predict(X))\n",
      "     |  [1 1 1 2 2 2]\n",
      "     |  >>> eclf3 = VotingClassifier(estimators=[\n",
      "     |  ...        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n",
      "     |  ...        voting='soft', weights=[2,1,1],\n",
      "     |  ...        flatten_transform=True)\n",
      "     |  >>> eclf3 = eclf3.fit(X, y)\n",
      "     |  >>> print(eclf3.predict(X))\n",
      "     |  [1 1 1 2 2 2]\n",
      "     |  >>> print(eclf3.transform(X).shape)\n",
      "     |  (6, 6)\n",
      "     |  >>>\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      VotingClassifier\n",
      "     |      sklearn.utils.metaestimators._BaseComposition\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, estimators, voting='hard', weights=None, n_jobs=1, flatten_transform=None)\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the estimators.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "     |          Training vectors, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples]\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples] or None\n",
      "     |          Sample weights. If None, then samples are equally weighted.\n",
      "     |          Note that this is supported only if all underlying estimators\n",
      "     |          support sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get the parameters of the VotingClassifier\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep: bool\n",
      "     |          Setting it to True gets the various classifiers and the parameters\n",
      "     |          of the classifiers as well\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "     |          Training vectors, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      ----------\n",
      "     |      maj : array-like, shape = [n_samples]\n",
      "     |          Predicted class labels.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Setting the parameters for the voting classifier\n",
      "     |      \n",
      "     |      Valid parameter keys can be listed with get_params().\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      params: keyword arguments\n",
      "     |          Specific parameters using e.g. set_params(parameter_name=new_value)\n",
      "     |          In addition, to setting the parameters of the ``VotingClassifier``,\n",
      "     |          the individual classifiers of the ``VotingClassifier`` can also be\n",
      "     |          set or replaced by setting them to None.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      # In this example, the RandomForestClassifier is removed\n",
      "     |      clf1 = LogisticRegression()\n",
      "     |      clf2 = RandomForestClassifier()\n",
      "     |      eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)]\n",
      "     |      eclf.set_params(rf=None)\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Return class labels or probabilities for X for each estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "     |          Training vectors, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      If `voting='soft'` and `flatten_transform=True`:\n",
      "     |        array-like = (n_classifiers, n_samples * n_classes)\n",
      "     |        otherwise array-like = (n_classifiers, n_samples, n_classes)\n",
      "     |          Class probabilities calculated by each classifier.\n",
      "     |      If `voting='hard'`:\n",
      "     |        array-like = [n_samples, n_classifiers]\n",
      "     |          Class labels predicted by each classifier.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  named_estimators\n",
      "     |  \n",
      "     |  predict_proba\n",
      "     |      Compute probabilities of possible outcomes for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "     |          Training vectors, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      ----------\n",
      "     |      avg : array-like, shape = [n_samples, n_classes]\n",
      "     |          Weighted average probability for each class per sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to X and y with optional parameters fit_params\n",
      "     |      and returns a transformed version of X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : numpy array of shape [n_samples, n_features]\n",
      "     |          Training set.\n",
      "     |      \n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : numpy array of shape [n_samples, n_features_new]\n",
      "     |          Transformed array.\n",
      "\n",
      "DATA\n",
      "    __all__ = ['BaseEnsemble', 'RandomForestClassifier', 'RandomForestRegr...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn.ensemble\n",
    "help(sklearn.ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
